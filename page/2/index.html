<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="https://tracyxinwang.github.io">
  <title>Xin Wang&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="CUHK在读博士生">
<meta name="keywords" content="大脑建模，前通信狗，程序媛">
<meta property="og:type" content="website">
<meta property="og:title" content="Xin Wang&#39;s Blog">
<meta property="og:url" content="https://tracyxinwang.github.io/page/2/index.html">
<meta property="og:site_name" content="Xin Wang&#39;s Blog">
<meta property="og:description" content="CUHK在读博士生">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Xin Wang&#39;s Blog">
<meta name="twitter:description" content="CUHK在读博士生">
  
    <link rel="alternative" href="/atom.xml" title="Xin Wang&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon1.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  
<!-- Google Analytics -->
<!--
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-103967152-3', 'auto');
ga('send', 'pageview');

</script> -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-103967152-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-103967152-3');
</script>

<!-- End Google Analytics -->


  
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?82e6534f81bd363aad63d87ffa506168";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/avatar.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/">Tracy Xin Wang</a></h1>
		</hgroup>
		
		<p class="header-subtitle">仅此一生，竭尽全力</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/archives">归档</a></li>
	        
				<li><a href="/tags/杂文/">杂文</a></li>
	        
				<li><a href="/tags">标签云</a></li>
	        
				<li><a href="/books">读书</a></li>
	        
				<li><a href="/">相册</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
		        
					<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
		        
					<a class="weixin" target="_blank" href="#" title="weixin"><i class="icon-weixin"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:wangxin@link.cuhk.edu.hk" title="mail"><i class="icon-mail"></i></a>
		        
					<a class="google" target="_blank" href="#" title="google"><i class="icon-google"></i></a>
		        
					<a class="linkedin" target="_blank" href="#" title="linkedin"><i class="icon-linkedin"></i></a>
		        
			</div>
		</nav>

		<nav class="header-nav">
		
			<script>
				fetch('https://v1.hitokoto.cn/?c=a')
					.then(function (res){
						return res.json();
					})
					.then(function (data) {
						var hitokoto = document.getElementById('hitokoto');
						hitokoto.innerText = '『' + data.hitokoto + '』——' + '「' + data.from + '」';
					})
					.catch(function (err) {
						console.error(err);
					});
			</script>
			<p id="hitokoto" style="font-size: 70%; margin-top: 140px">:D fetching...</p>
    	
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/avatar.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author">Tracy Xin Wang</h1>
			</hgroup>
			
			<p class="header-subtitle"><i class="icon icon-quo-left"></i>仅此一生，竭尽全力<i class="icon icon-quo-right"></i></p>
			
			
			
				
			
				
			
				
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
			        
						<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
			        
						<a class="weixin" target="_blank" href="#" title="weixin"><i class="icon-weixin"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:wangxin@link.cuhk.edu.hk" title="mail"><i class="icon-mail"></i></a>
			        
						<a class="google" target="_blank" href="#" title="google"><i class="icon-google"></i></a>
			        
						<a class="linkedin" target="_blank" href="#" title="linkedin"><i class="icon-linkedin"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 16.666666666666668%"><a href="/">主页</a></li>
		        
					<li style="width: 16.666666666666668%"><a href="/archives">归档</a></li>
		        
					<li style="width: 16.666666666666668%"><a href="/tags/杂文/">杂文</a></li>
		        
					<li style="width: 16.666666666666668%"><a href="/tags">标签云</a></li>
		        
					<li style="width: 16.666666666666668%"><a href="/books">读书</a></li>
		        
					<li style="width: 16.666666666666668%"><a href="/">相册</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-coursera_motion_pl03" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/21/coursera_motion_pl03/">Coursera Motion Planning for Self-Driving Cars 03</a>
    </h1>
  

        
        <a href="/2019/08/21/coursera_motion_pl03/" class="archive-article-date">
  	<time datetime="2019-08-21T04:30:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-08-21</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!--  
            <div id="toc" class="toc-article">
              <strong class="toc-title">文章目录</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Mission-Planning-in-Driving-Environments"><span class="toc-number">1.</span> <span class="toc-text">Mission Planning in Driving Environments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Creating-a-Road-Network-Graph"><span class="toc-number">1.1.</span> <span class="toc-text">Creating a Road Network Graph</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dijkstra’s-Shortest-Path-Search"><span class="toc-number">1.2.</span> <span class="toc-text">Dijkstra’s Shortest Path Search</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Shortest-Path-Search"><span class="toc-number">1.3.</span> <span class="toc-text">A* Shortest Path Search</span></a></li></ol></li></ol>
            </div>
           -->
        <!--  
    <div id="toc-container"> 
        <div id="toc"> 
            <p> 
                <strong>文章目录</strong>
            </p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Mission-Planning-in-Driving-Environments"><span class="toc-text">Mission Planning in Driving Environments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Creating-a-Road-Network-Graph"><span class="toc-text">Creating a Road Network Graph</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dijkstra’s-Shortest-Path-Search"><span class="toc-text">Dijkstra’s Shortest Path Search</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Shortest-Path-Search"><span class="toc-text">A* Shortest Path Search</span></a></li></ol></li></ol>
        </div>
    </div>
 -->
        <p>From Coursera, State Estimation and Localization for Self-Driving Cars by University of Toronto<br><a href="https://www.coursera.org/learn/motion-planning-self-driving-cars" target="_blank" rel="noopener">https://www.coursera.org/learn/motion-planning-self-driving-cars</a></p>
<h2 id="Mission-Planning-in-Driving-Environments"><a href="#Mission-Planning-in-Driving-Environments" class="headerlink" title="Mission Planning in Driving Environments"></a>Mission Planning in Driving Environments</h2><h3 id="Creating-a-Road-Network-Graph"><a href="#Creating-a-Road-Network-Graph" class="headerlink" title="Creating a Road Network Graph"></a>Creating a Road Network Graph</h3><p>Graph:</p>
<ul>
<li>The edges is defined by the segments of the road that connect each sample point according to the rules of the road.</li>
<li>the edges of the graph are directed</li>
<li>When the graph is unweighted, a good candidate algorithm is the Breadth-First Search or BFS.</li>
</ul>
<p>BFS:</p>
<ul>
<li>At a high level, BFS can be thought of as iterating through all of the vertices in the graph but doing so in a manner such that all adjacent vertices are evaluated first before proceeding deeper into the graph.</li>
</ul>
<h3 id="Dijkstra’s-Shortest-Path-Search"><a href="#Dijkstra’s-Shortest-Path-Search" class="headerlink" title="Dijkstra’s Shortest Path Search"></a>Dijkstra’s Shortest Path Search</h3><p>Dijkstra’s Algorithm </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Algorithm Dijkstra<span class="string">'s (G,s,t)</span></span><br><span class="line"><span class="string">    open &lt;- MinHeap()</span></span><br><span class="line"><span class="string">    closed &lt;- Set()</span></span><br><span class="line"><span class="string">    predecessors &lt;- Dict()</span></span><br><span class="line"><span class="string">    open.push(s.0)</span></span><br><span class="line"><span class="string">    while !open.isEmpty() do</span></span><br><span class="line"><span class="string">        if isGoal(u) then</span></span><br><span class="line"><span class="string">            return extractPath(u, predecessors)</span></span><br><span class="line"><span class="string">        for all v $\in$ u.successors() </span></span><br><span class="line"><span class="string">            if v $\in$ closed then</span></span><br><span class="line"><span class="string">                continue</span></span><br><span class="line"><span class="string">            uvCost &lt;- edgeCost(G,u,v)</span></span><br><span class="line"><span class="string">            if v $\in$ open then</span></span><br><span class="line"><span class="string">                if uCost + uvCost &lt; open[v] then</span></span><br><span class="line"><span class="string">                    open[v] &lt;- uCost + uvCost</span></span><br><span class="line"><span class="string">                    predecessors[v] &lt;- u</span></span><br><span class="line"><span class="string">            else</span></span><br><span class="line"><span class="string">                open.push(v, uCost + uvCost)</span></span><br><span class="line"><span class="string">                predecessors[v] &lt;- u</span></span><br><span class="line"><span class="string">        closed add(u)</span></span><br></pre></td></tr></table></figure>
<h3 id="A-Shortest-Path-Search"><a href="#A-Shortest-Path-Search" class="headerlink" title="A* Shortest Path Search"></a>A* Shortest Path Search</h3><p>Heuristic:</p>
<ul>
<li>a search heuristic is an estimate of the remaining cost to reach the destination vertex from any given vertex in the graph. </li>
<li>in this case, a useful estimate on the cost or length between any two vertices is the straight line or Euclidean distance between them</li>
</ul>
<p><strong>Euclidean Heuristic</strong></p>
<ul>
<li>Exploits structure of the problem</li>
<li>Fast to calculate</li>
<li>Straight-line distance between two vertices is a useful estimate of true distance along the graph<br>$$h(v) = |t-v|$$</li>
</ul>
<p><img src="/images/motion_pl02.jpg" width="80%" height="80%"></p>
<p>Extensions to Other Factors</p>
<ul>
<li>Traffic, speed limits, and weather affect mission planning</li>
<li>Time rather than distance is better at capturing these factors</li>
<li>Replace distance edge weights with time estimates</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Learning</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Self-driving</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2019/08/21/coursera_motion_pl03/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-coursera_motion_pl02" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/19/coursera_motion_pl02/">Coursera Motion Planning for Self-Driving Cars 02</a>
    </h1>
  

        
        <a href="/2019/08/19/coursera_motion_pl02/" class="archive-article-date">
  	<time datetime="2019-08-19T13:30:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-08-19</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!--  
            <div id="toc" class="toc-article">
              <strong class="toc-title">文章目录</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Mapping-for-Planning"><span class="toc-number">1.</span> <span class="toc-text">Mapping for Planning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Occupancy-Grids"><span class="toc-number">1.1.</span> <span class="toc-text">Occupancy Grids</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Populating-Occupancy-Grids-from-LIDAR-Scan-Data-Part-1"><span class="toc-number">1.2.</span> <span class="toc-text">Populating Occupancy Grids from LIDAR Scan Data (Part 1)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Populating-Occupancy-Grids-from-LIDAR-Scan-Data-Part-2"><span class="toc-number">1.3.</span> <span class="toc-text">Populating Occupancy Grids from LIDAR Scan Data (Part 2)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Occupancy-Grid-Updates-for-Self-Driving-Cars"><span class="toc-number">1.4.</span> <span class="toc-text">Occupancy Grid Updates for Self-Driving Cars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#High-Definition-Road-Maps"><span class="toc-number">1.5.</span> <span class="toc-text">High Definition Road Maps</span></a></li></ol></li></ol>
            </div>
           -->
        <!--  
    <div id="toc-container"> 
        <div id="toc"> 
            <p> 
                <strong>文章目录</strong>
            </p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Mapping-for-Planning"><span class="toc-text">Mapping for Planning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Occupancy-Grids"><span class="toc-text">Occupancy Grids</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Populating-Occupancy-Grids-from-LIDAR-Scan-Data-Part-1"><span class="toc-text">Populating Occupancy Grids from LIDAR Scan Data (Part 1)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Populating-Occupancy-Grids-from-LIDAR-Scan-Data-Part-2"><span class="toc-text">Populating Occupancy Grids from LIDAR Scan Data (Part 2)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Occupancy-Grid-Updates-for-Self-Driving-Cars"><span class="toc-text">Occupancy Grid Updates for Self-Driving Cars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#High-Definition-Road-Maps"><span class="toc-text">High Definition Road Maps</span></a></li></ol></li></ol>
        </div>
    </div>
 -->
        <p>From Coursera, State Estimation and Localization for Self-Driving Cars by University of Toronto<br><a href="https://www.coursera.org/learn/motion-planning-self-driving-cars" target="_blank" rel="noopener">https://www.coursera.org/learn/motion-planning-self-driving-cars</a></p>
<h2 id="Mapping-for-Planning"><a href="#Mapping-for-Planning" class="headerlink" title="Mapping for Planning"></a>Mapping for Planning</h2><p>two environmental maps: the occupancy grid map and the high-definition road map.</p>
<h3 id="Occupancy-Grids"><a href="#Occupancy-Grids" class="headerlink" title="Occupancy Grids"></a>Occupancy Grids</h3><ul>
<li>discretized fine grain grid which surrounds the current ego vehicle position.<ul>
<li>can be 2D or 3D</li>
</ul>
</li>
<li>Each grid square of the occupancy grid indicates if a static or stationary object is present in that grid location. If so, that grid location is classified as occupied. <ul>
<li>Trees and buildings</li>
<li>Curbs and other non drivable surfaces: lawns or sidewalks. </li>
</ul>
</li>
<li>Each cell is a binary value denoted by $m^i \in {0,1}$, where 1 indicates that the square is occupied by a static object, and 0 indicates that it is not.</li>
<li>Assumption of Occupancy Grid<ul>
<li>Static environment: all dynamic objects or moving objects must be removed from the sensor data before it is used for occupancy grid mapping</li>
<li>Independence of each cell: to simplify the update functions needed to create the occupancy grid</li>
<li>Known vehicle state at each time step</li>
</ul>
</li>
<li>LIDAR sensor is used most frequently<ul>
<li>Several components of the LIDAR data need to be filtered out before this data can be used to construct an occupancy grid. </li>
<li>filter all LIDAR points which comprise the ground plane. In this case, the ground plane is the road surface which the autonomous car can safely drive on. </li>
<li>filter all points which appear above the highest point of the vehicle: they can be ignored as they will not impede the progression of the autonomous vehicle. </li>
<li>filter all non-static objects which had been captured by the LIDAR. This includes all vehicles, pedestrians, bicycles, and animals. </li>
<li>Once all filtering of the LIDAR data is complete, the 3D LIDAR data will need to be projected down to a 2D plane to be used to construct our occupancy grid. </li>
</ul>
</li>
<li>To handle the environmental and sensor noise, the occupancy grid will be modified to be probabilistic. <ul>
<li>the occupancy grid can now be represented as a belief map: $bel_t(m^i) = p(m^i|(y,x))$</li>
<li>$m^i$ represents a single square of the occupancy grid, where i can be constructed from measurements y, and the vehicle location x. </li>
<li>$m^i$ is equal to the probability that the current cell $m^i$ is occupied given the sensor measurements gathered for that cell location.</li>
<li>Threshold of certainty will be used to establish occupancy to get the binary map</li>
</ul>
</li>
<li>Bayesian Update of the Occupancy Grid<ul>
<li>To improve robustness multiple timesteps are used to produce the current map: $bel_t(m^i) = p(m^i|(y,x)_{1:t})$</li>
<li>we can update beliefs in a recursive manner so that at each time step t, we use all prior information from time one onwards to define our belief.</li>
<li>Bayes’theorem is applied for at each update step for each cell: $bel_t(m^i) = \eta p(y_t|m^i) bel_{t-1}(m^i)$</li>
<li>The distribution $p(y_t|m^i)$, is the probability of getting a particular measurement given a cell $m^i$ is occupied. This is known as the measurement model</li>
<li>The belief at time t-1 $bel_{t-1}(m^i)$ corresponds to the prior belief stored in our occupancy grid from the previous time step. </li>
<li>rely on the Markov assumption, that all necessary information for estimating cell occupancy is captured in the belief map at each time step. So no earlier history needs to be considered in the cell update equations.</li>
<li>$\eta$ in this case corresponds to a normalizing constant applied to the belief map. </li>
</ul>
</li>
</ul>
<h3 id="Populating-Occupancy-Grids-from-LIDAR-Scan-Data-Part-1"><a href="#Populating-Occupancy-Grids-from-LIDAR-Scan-Data-Part-1" class="headerlink" title="Populating Occupancy Grids from LIDAR Scan Data (Part 1)"></a>Populating Occupancy Grids from LIDAR Scan Data (Part 1)</h3><p><strong>lssue With Standard Bayesian Update</strong></p>
<ul>
<li>Multiplication of numbers close to zero is hard for computers: lead to significant rounding error when multiplying small numbers, which in turn can lead to instability in the estimate of the probabilities.</li>
<li>The multiplication of probabilities turns out to be an inefficient way to perform the belief update. </li>
<li>Solution: Instead of storing the belief map with the values ranging from 0-1, we can convert the beliefs into log odds probabilities using the logit function: $log(\frac{p}{1-p})$</li>
<li>This leads to cell values ranging from $-/infty$ to $+/infty$ avoiding the issue with numbers close to zero.</li>
</ul>
<p><strong>Bayesian Log Odds Single Cell Update Derivation</strong></p>
<ul>
<li>Applying Bayes’rule: $$p(m^i|y_{1:t})$$<ul>
<li>where $m^i$ is the current occupancy grid map square at location $i$ and $y_{1:t}$ are the sensor measurements of that cell from time one to time t. </li>
</ul>
</li>
<li>The full Bayesian update for incorporating the latest measurements into our occupancy belief: $$p(m^i|y_{1:t}) = \frac{p(y_t|y_{1:t-1},m^i)p(m^i|y_{1:t-1}}{p(y_t|y_{1:t-1})} $$</li>
<li>Next applying the Markov assumption: $$p(m^i|y_{1:t}) = \frac{p(y_t|m^i)p(m^i|y_{1:t-1})}{p(y_t|y_{1:t-1})}$$<ul>
<li>this ensures the current measurement is independent of previous measurements if the map state $m^i$ is known. </li>
</ul>
</li>
<li>Next is to expand the measurement model $p(y_t|m^i)$ by the application of Bayes’ rule once again $$p(y_t|m^i) = \frac{p(m^i|y_t)p(y_t)}{p(m^i)} $$</li>
<li>Substitute the expanded measurement model in blue into the main Bayesian inference equation:<br>$$p(m^i|y_{1:t}) = \frac{p(m^i|y_t)p(y_t)p(m^i|y_{1:t-1})}{p(m^i)p(y_t|y_{1:t-1})}$$</li>
<li>After logit function: $$logit(p(m^i|y_{1:t})) = logit(p(m^i|y_t)) + logit(p(m^i|y_{1:t-1})) - logit(p(m^i)) $$<ul>
<li>rewrite it as: $l_{t,i} = logit(p(m^i|y_t)) + l_{t-1,i} - l_{0,i}$ </li>
<li>$l_{t-1,i}$ is the previous belief</li>
<li>$l_{0,i}$ is the initial belief</li>
</ul>
</li>
<li>Advantages over directly updating probabilities<ul>
<li>Numerically stable</li>
<li>Computationally efficient</li>
</ul>
</li>
</ul>
<h3 id="Populating-Occupancy-Grids-from-LIDAR-Scan-Data-Part-2"><a href="#Populating-Occupancy-Grids-from-LIDAR-Scan-Data-Part-2" class="headerlink" title="Populating Occupancy Grids from LIDAR Scan Data (Part 2)"></a>Populating Occupancy Grids from LIDAR Scan Data (Part 2)</h3><p><strong>Inverse Measurement Module</strong></p>
<ul>
<li>Relative change: $$r^i = \sqrt((m^i_x - x_{1,t})^2 + (m^i_y - x_{2,t})^2)$$<ul>
<li>the Euclidean distance from the sensor to the cell</li>
<li>where $r^i$ is the range to grid cell i </li>
<li>$m^i_x$ and $m^i_y$ are the x and y coordinates of the center of the grid cell.</li>
<li>$x_{1,t}$ and $x_{2,t}$ are the sensor location at the current time t</li>
</ul>
</li>
<li>Relative bearing: $$\phi^i = \tan^{-1} (\frac{m^i_y - x_{2,t}}{m^i_x - x_{1,t}}) - x_{3,t}$$</li>
<li>Closest relative bearing: $$k = argmin(|\phi^i - \phi^s_j|)$$<ul>
<li>For each cell, we associate the most relevant lidar beam by finding the measurement with the minimum error between its beam angle and the cell bearing.</li>
</ul>
</li>
<li>Then define two parameters $\alpha$ and $\beta$, which define a sector around each beam in which cell occupancy should be determined based on the beam range.<ul>
<li>$\alpha$ defines the affected range for high probability</li>
<li>$\beta$ defines the affected angle for low and high probability</li>
<li>This essentially creates a region around each beam which will get assigned the measurement information of that particular beam.</li>
</ul>
</li>
<li>We are now ready to assign a probability that any cell is occupied given the lidar measurements received based on these three types of cells:<ul>
<li>no information zone: if $r^i &gt; min(r^s_{max}) $ or $ |\phi^i -\phi^s_k| &gt; \beta/2$</li>
<li>high information zone: if $r^s_k &lt; r^s_{max}$ and $|r^i - r^s_k| &gt; \alpha/2$</li>
<li>low information zone: if $r_i &lt; r^s_k$</li>
</ul>
</li>
</ul>
<p><strong>Inverse Measurement Module With Ray Tracing</strong></p>
<ul>
<li>However, this simple inverse measurement model can be computationally expensive:<ul>
<li>It requires a full update of every cell in the measurement map</li>
<li>and relies on multiple floating-point operations to identify which measurements correspond to which cells. </li>
</ul>
</li>
<li>Alternative is to use Ray tracing algorithm <ul>
<li>using Bresenham’s line algorithm</li>
<li>Rasterized line algorithm</li>
<li>Uses very cheap fixed point operations for fast calculations</li>
</ul>
</li>
</ul>
<h3 id="Occupancy-Grid-Updates-for-Self-Driving-Cars"><a href="#Occupancy-Grid-Updates-for-Self-Driving-Cars" class="headerlink" title="Occupancy Grid Updates for Self-Driving Cars"></a>Occupancy Grid Updates for Self-Driving Cars</h3><p>Downsampling</p>
<ul>
<li>Up to ~1.2 million points per second</li>
<li>Removal of redundant points</li>
<li>Improves computation</li>
</ul>
<p>Removal of overhanging objects</p>
<ul>
<li>Removing all Lidar points that are above a given threshold of the height limit of the car</li>
</ul>
<p>Removal of ground plane</p>
<ul>
<li>Difficult to estimate due to several complications o Differing road geometries oCurbs,lane boundaries oDon’t want to miss small objects</li>
<li>Ground plane Classification<ul>
<li>Utilize segmentation to remove points of road elements</li>
<li>Keep points from no drivable surfaces</li>
</ul>
</li>
</ul>
<p>Removal of Dynamic Objects</p>
<ul>
<li>Not all vehicles are dynamic, so they should be included</li>
<li>History of dynamic object location can be used to identify parked vehicle</li>
<li>The dynamic objects are identified from the previous LIDAR frame</li>
<li>Predicted future location improvement</li>
</ul>
<p>Projection of LIDAR Onto a 2D Plane</p>
<ul>
<li>Collapse all points by Zeroing the Z coordinate</li>
<li>Sum up the number of LIDAR points in each grid location<ul>
<li>More points indicated greater chance of occupation of that grid cell</li>
</ul>
</li>
</ul>
<h3 id="High-Definition-Road-Maps"><a href="#High-Definition-Road-Maps" class="headerlink" title="High Definition Road Maps"></a>High Definition Road Maps</h3><p>High Definition Road Maps stores </p>
<ul>
<li>the precise road locations including all lanes down to a few centimeters accuracy. </li>
<li>all of the locations of road signs and signals which might effect the autonomous vehicle.</li>
</ul>
<p><strong>Lanelet map</strong></p>
<ul>
<li>Due to the detailed and interconnected nature of the data, an effective method is required to store all information contained within the map.</li>
<li>Due to this method’s effectiveness in storage and communication of the complex set of information needed for HD mapping, it is widely used</li>
<li>The lanelet map has two main components<ul>
<li>Lanelet element: stores all information connected to a small longitudinal segment of a lane on a road which it represents</li>
<li>Intersection element: stores all lanelet elements which are part of a single intersection for simple retrieval during motion planning tasks. </li>
</ul>
</li>
</ul>
<p><strong>Lanelet element</strong></p>
<ul>
<li>Defines the following:<ul>
<li>Left and Right Boundaries</li>
<li>Regulation<ul>
<li>Elements: e.g. a stop sign line or a static sign line</li>
<li>Attributes: attributes that might affect that segment of the road e.g. speed limit</li>
<li>Note that we only store a line for any regulatory element as this is the point that the autonomous vehicle treats as the active location for that regulatory element.</li>
</ul>
</li>
<li>Connectivity to other lanelets<ul>
<li>This allows for easy traversal and calculations through the graph created by the set of lanelets in an HD map.</li>
</ul>
</li>
</ul>
</li>
<li>A new lanelet is created when a new regulatory element is encountered or ends<ul>
<li>Boundaries of the lanelet are represented as the edges of the lanelet.</li>
<li>These boundaries capture either marked lane boundaries or curves which inhibit driving. </li>
<li>Lane boundaries are stored as a set of points creating a continuous polygonal line. Each point stores its x, y, and z GPS coordinates. The distance between points can be as fine as a few centimeters, or as course as a few meters depending on the smoothness of the polyline in question. </li>
<li>The ordering of the points defines the direction of travel and heading for the lanelet.</li>
</ul>
</li>
<li>The entire lanelet structure is connected in a directed graph, which is the base structure of the HD map</li>
<li>Operations Done On Lanelets<ul>
<li>Path planning through complex road networks</li>
<li>Localize Dynamic Objects</li>
<li>Interactions with other Dynamic Objects</li>
</ul>
</li>
</ul>
<p><strong>Creations Of Lanelets</strong><br>3 methods to create the lanelet map</p>
<ul>
<li>Offline creation</li>
<li>Online creation: highly computationally expensive</li>
<li>Offline creation with online updating</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Learning</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Self-driving</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2019/08/19/coursera_motion_pl02/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-coursera_motion_pl01" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/16/coursera_motion_pl01/">Coursera Motion Planning for Self-Driving Cars 01</a>
    </h1>
  

        
        <a href="/2019/08/16/coursera_motion_pl01/" class="archive-article-date">
  	<time datetime="2019-08-16T13:30:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-08-16</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!--  
            <div id="toc" class="toc-article">
              <strong class="toc-title">文章目录</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Planning-Problem"><span class="toc-number">1.</span> <span class="toc-text">The Planning Problem</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Driving-Missions-Scenarios-and-Behaviour"><span class="toc-number">1.1.</span> <span class="toc-text">Driving Missions, Scenarios, and Behaviour</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Motion-Planning-Constraints"><span class="toc-number">1.2.</span> <span class="toc-text">Motion Planning Constraints</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Objective-Functions-for-Autonomous-Driving"><span class="toc-number">1.3.</span> <span class="toc-text">Objective Functions for Autonomous Driving</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hierarchical-Motion-Planning"><span class="toc-number">1.4.</span> <span class="toc-text">Hierarchical Motion Planning</span></a></li></ol></li></ol>
            </div>
           -->
        <!--  
    <div id="toc-container"> 
        <div id="toc"> 
            <p> 
                <strong>文章目录</strong>
            </p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Planning-Problem"><span class="toc-text">The Planning Problem</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Driving-Missions-Scenarios-and-Behaviour"><span class="toc-text">Driving Missions, Scenarios, and Behaviour</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Motion-Planning-Constraints"><span class="toc-text">Motion Planning Constraints</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Objective-Functions-for-Autonomous-Driving"><span class="toc-text">Objective Functions for Autonomous Driving</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hierarchical-Motion-Planning"><span class="toc-text">Hierarchical Motion Planning</span></a></li></ol></li></ol>
        </div>
    </div>
 -->
        <p>From Coursera, State Estimation and Localization for Self-Driving Cars by University of Toronto<br><a href="https://www.coursera.org/learn/motion-planning-self-driving-cars" target="_blank" rel="noopener">https://www.coursera.org/learn/motion-planning-self-driving-cars</a></p>
<h2 id="The-Planning-Problem"><a href="#The-Planning-Problem" class="headerlink" title="The Planning Problem"></a>The Planning Problem</h2><h3 id="Driving-Missions-Scenarios-and-Behaviour"><a href="#Driving-Missions-Scenarios-and-Behaviour" class="headerlink" title="Driving Missions, Scenarios, and Behaviour"></a>Driving Missions, Scenarios, and Behaviour</h3><p><strong>Autonomous Driving Mission</strong></p>
<ul>
<li>Mission is to navigate from point A to point B on the map</li>
<li>Mission planning is higher-level planning</li>
<li>Low-level details are abstracted away</li>
<li>Goal is to find most efficient path (in terms of time or distance)</li>
</ul>
<p><strong>Road Structure Scenarios</strong></p>
<ul>
<li>Road structure influences driving scenario through lane boundaries and regulatory elements</li>
<li>Simplest case is driving straight, following the center of the lane<ul>
<li>Minimize deviation from centerline</li>
<li>Attain reference speed for efficiency</li>
</ul>
</li>
<li>Lane changes are more complex<ul>
<li>Different shapes for different situations</li>
<li>Shape depends on vehicle speed, acceleration limitations</li>
<li>Time horizon of execution affects the aggressiveness of the lane change</li>
</ul>
</li>
<li>Left and right turn scenarios are common in intersections and drivelanes<ul>
<li>Shape of turn varies, similar to lane changes</li>
<li>State of surrounding environment impacts the ability of the vehicle to make turns</li>
</ul>
</li>
<li>U-turns are useful for efficient direction changes<ul>
<li>Shape of U-turn will depend on car’s speed and acceleration limits</li>
<li>Not always possible at all intersections</li>
</ul>
</li>
</ul>
<p><strong>Obstacle Scenarios</strong></p>
<ul>
<li>Static and dynamic obstacles also impact the driving scenario</li>
<li>Static obstacles restrict which locations our path can occupy</li>
<li>Most important dynamic obstacle is often the leading vehicle in front of the ego vehicle<ul>
<li>Need to maintain time gap for safety</li>
</ul>
</li>
<li>Dynamic obstacles impact turns/lane changes as well</li>
<li>Depending on locations and speed, differenttime windows of execution are available for the autonomous vehicle</li>
<li>Need to use estimation and prediction to calculate these windows of opportunity</li>
<li>Different dynamic obstacles in the scenario have different characteristics and behaviours</li>
</ul>
<p><strong>Behaviours</strong></p>
<ul>
<li>Speed Tracking</li>
<li>Decelerate to Stop</li>
<li>Stay Stopped</li>
<li>Yield</li>
<li>Emergency Stop<ul>
<li>Not an exhaustive list</li>
</ul>
</li>
</ul>
<p><strong>Hierarchical Planning Introduction</strong></p>
<ul>
<li>Driving mission and scenarios are complex problems</li>
<li>Break them into a hierarchy of optimization problems</li>
<li>Each optimization problem tailored to the correct scope and level of abstraction</li>
<li>Higher in the hierarchy means more abstraction</li>
<li>Each optimization problem will have constraints and objective functions<br><img src="/images/motion_pl01.jpg" width="80%" height="80%"></li>
</ul>
<h3 id="Motion-Planning-Constraints"><a href="#Motion-Planning-Constraints" class="headerlink" title="Motion Planning Constraints"></a>Motion Planning Constraints</h3><p><strong>Constrains from vehicle’s kinematics and dynamics</strong></p>
<p>Bicycle Model</p>
<ul>
<li>Kinematics simplified to bicycle model</li>
<li>Bicycle model imposes curvature constraint on our path planning process</li>
<li>$\dot \theta = \frac{V \tan(\delta)}{L}$</li>
<li>$|\kappa \le \kappa_{max}|$<ul>
<li>there is a maximum magnitude of curvature that can be executed when traversing a given path. </li>
</ul>
</li>
<li>Curvature constraint is non-holonomic<ul>
<li>Non-holonomic constraints reduce the directions a mobile robot can travel at any point</li>
<li>Makes motion planning challenging</li>
<li>curvature computation: $\kappa = \frac{x’y’’ - y’x’’}{(x’^2+y’^2)^{3/2}}$</li>
</ul>
</li>
</ul>
<p>Vehicle Dynamics</p>
<ul>
<li>Recall: friction ellipse denotes maximum magnitude of tire forces before stability loss</li>
<li>Friction forces are extreme limit; more useful constraint is accelerations tolerable by passengers <ul>
<li>Given by “comfort rectangle”range of lateral and longitudinal accelerations</li>
</ul>
</li>
</ul>
<p>Dynamics and Curvature</p>
<ul>
<li>Friction limits and comfort restrict lateral acceleration<ul>
<li>Lateral acceleration is a function of instantaneous turning radius of<br>path and velocity</li>
<li>$a_{lat} = \frac{v^2}{r}, \quad a_{lat} \le a_{lat_{max}}$</li>
</ul>
</li>
<li>Recall: instantaneous curvature is inverse of instantaneous turning radius</li>
<li>Substituting, velocity is constrained by path curvature and lateral acceleration<ul>
<li>$v^2 \le \frac{a_{lat_{max}}}{\kappa}$</li>
</ul>
</li>
</ul>
<p><strong>Constrains from static and dynamic obstacles</strong><br>Static Obstacles</p>
<ul>
<li>Static obstacles block portions of workspace<ul>
<li>Occupancy grid encoding stores obstacle locations</li>
</ul>
</li>
<li>Static obstacle constraints satisfied by performing collision checking<ul>
<li>Can check for collisions using the swath of the vehicle’s path</li>
<li>Can also check for closest obstacle along ego vehicle’s path</li>
</ul>
</li>
</ul>
<p>Dynamic Obstacles</p>
<ul>
<li>dynamic obstacles will constrain both our behavior planning process, where we make maneuver decisions, as well as the local planning process, where it will affect our velocity profile planning.</li>
</ul>
<p><strong>Impact of regulatory elements</strong> </p>
<ul>
<li>Lane constraints restrict path locations</li>
<li>Signs, traffic lights influence vehicle behaviour</li>
</ul>
<h3 id="Objective-Functions-for-Autonomous-Driving"><a href="#Objective-Functions-for-Autonomous-Driving" class="headerlink" title="Objective Functions for Autonomous Driving"></a>Objective Functions for Autonomous Driving</h3><p><strong>Efficiency</strong></p>
<ul>
<li>Path length:<ul>
<li>Minimize the arc length of a path to generate the shortest path to the goal</li>
<li>the arc length of the path is the total accumulated distance the car will travel as it traverses the path. </li>
<li>$s_f = \int^{x_f}_{x_i} \sqrt{1+(\frac{dy}{dx})^2} dx$</li>
</ul>
</li>
<li>Travel time:<ul>
<li>Minimize time to destination while following the planned path</li>
<li>$T_f = \int^{s_f}_0 \frac {1}{v(s)} ds$</li>
</ul>
</li>
</ul>
<p><strong>Reference Tracking</strong></p>
<ul>
<li>In path planning for autonomous driving, we may be given a reference path we’d like to follow</li>
<li>we can penalize deviation from the reference path or speed profile</li>
<li>$\int^{s_f}<em>0 |x(s) - x</em>{ref}(s)| ds$</li>
<li>$\int^{s_f}<em>0 |v(s) - v</em>{ref}(s)| ds$</li>
<li>For velocity: Hinge loss to penalize speed limit violations severely</li>
<li>$\int^{s_f}<em>0 (v(s) - v</em>{ref}(s))_+ ds$</li>
</ul>
<p><strong>Smoothness</strong><br>$$\int^{s_f}_0 | \dddot x(s) |^2 ds$$</p>
<ul>
<li>focus to minimize the jerk along our trajectory</li>
<li>Jerk is the rate of change of acceleration with respect to time, or the third derivative of position. </li>
<li>The jerk along the car’s trajectory greatly impacts the user’s comfort while in the car. So when planning our velocity profile, we would like to keep the accumulated absolute value of jerk as small as possible.</li>
</ul>
<h3 id="Hierarchical-Motion-Planning"><a href="#Hierarchical-Motion-Planning" class="headerlink" title="Hierarchical Motion Planning"></a>Hierarchical Motion Planning</h3><p>Hierarchical Planner</p>
<ul>
<li>Motion planning broken into hierarchy of subproblems</li>
<li>Mission planner: the highest level, focuses on map-level navigation</li>
<li>Behavioural planner focuses on other agents, rules of the road, driving behaviours</li>
<li>Local planner focuses on generating feasible, collision-free paths</li>
</ul>
<p><strong>Mission Planner</strong></p>
<ul>
<li>Highest level planner</li>
<li>Focuses on autonomous driving mission<ul>
<li>Navigate to destination at the map level</li>
</ul>
</li>
<li>Abstract away lower level details</li>
<li>Can be solved with graph-based methods (Dijkstra’s, A*)</li>
</ul>
<p><strong>Behavioural Planner</strong></p>
<ul>
<li>Behavioural planner decides when it is safe to proceed</li>
<li>Takes pedestrians, vehicles, cyclists into consideration</li>
<li>Also looks at regulatory elements, such as traffic lights and stop signs</li>
<li>Finite State Machines<ul>
<li>Composed of states and transitions </li>
<li>States are based on perception of surroundings </li>
<li>Transitions are based on inputs to the driving scenario (e.g.stop lights changing colour)</li>
<li>FSM is memoryless: Transitions only depend on input and current state, and not on past state sequence</li>
</ul>
</li>
<li>Rule-based System<ul>
<li>Rule-based systems use a hierarchy of rules to determine output behaviour</li>
<li>Rules are evaluated based on logical predicates<ul>
<li>Higher priority rules have precedence</li>
</ul>
</li>
<li>Example scenario with two rules<ul>
<li>green light A intersection → drive straight</li>
<li>pedestrian A driving straight → emergency stop</li>
</ul>
</li>
</ul>
</li>
<li>Reinforcement learning<ul>
<li>$R = \sum^\infty_{t=0} \gamma^t R_{a_t}(s_t, s_{t+1})$</li>
</ul>
</li>
</ul>
<p><strong>Local Planner</strong></p>
<ul>
<li>Local planning generates feasible, collision-free paths and comfortable velocity profiles</li>
<li>Decomposed into <em>path planning</em> and <em>velocity profile generation</em></li>
</ul>
<p>Path planner:</p>
<ul>
<li>The key ingenuity behind developing a good path planning algorithm is reducing the search space for optimization</li>
<li>Three main categories of path planners: sampling-based planners, variational planners and lattice planners</li>
<li>Sampling-based Planners:<ul>
<li>Randomly sample the control inputs to quickly explore the workspace</li>
<li>One of the most iconic sampling-based algorithms is the Rapidly Exploring Random Tree or RRT</li>
<li>These algorithms construct the branches of the tree of paths by generating points in randomly sampled locations and planning a path to the point, from the nearest point in the tree. </li>
<li>If the path is free from collisions with any static obstacles, that path is added to the tree. </li>
<li>This tree quickly explores the workspace with many potential paths and when a goal region is reached, the path that terminates in that region is returned.</li>
<li>Often very fast, but can generate poor-quality paths</li>
</ul>
</li>
<li>Variational planners:<ul>
<li>rely on the calculus of variations to optimize a trajectory function which maps points in time to positions in the workspace according to some cost-functional that takes obstacles and robot dynamics into consideration. $min_{\delta x} J(x+\delta x)$</li>
<li>are usually trajectory planners, which means they combine both path planning and velocity planning into a single-step.</li>
<li>Can be slower, and less likely to converge to a feasible solution</li>
<li>a variational method is the chomp algorithm</li>
</ul>
</li>
<li>Lattice Planners<ul>
<li>Constrain the search space by limiting actions available to the robot <ul>
<li>Set of actions known as control set</li>
</ul>
</li>
<li>This control set, when paired with a discretization of the workspace, implicitly defines a graph. This graph can then be searched using a graph search algorithm such as Dijkstra’s or A*, which results in fast computation of paths</li>
<li>While the lattice planner is often quite fast, the quality of paths are sensitive to the selected control set. </li>
<li>A common variance on the lattice planner is known as the conformal lattice planner, where goal points are selected some distance ahead of the car, laterally offset from one another with respect to the direction of the road and a path is optimized to each of these points. </li>
<li>Conformal lattice planner fits the control actions to the road structure</li>
</ul>
</li>
</ul>
<p>Velocity profile generation:</p>
<ul>
<li>is usually set up as a constrained optimization problem. </li>
<li>we would combine many of the velocity profile objectives such as the goals of minimizing jerk or minimizing deviation from a desired reference, the rectangle of comfortable acceleration</li>
<li>Once the objectives and constraints are formalized, it becomes a matter of solving the problem efficiently. One way to do this is to calculate convex approximations to the optimization domain and objectives, which helps ensure that our optimizer doesn’t get stuck in local minima.</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Learning</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Self-driving</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2019/08/16/coursera_motion_pl01/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-coursera_visual_percep05" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/coursera_visual_percep05/">Coursera Visual Perception for Self-Driving Cars 05</a>
    </h1>
  

        
        <a href="/2019/08/08/coursera_visual_percep05/" class="archive-article-date">
  	<time datetime="2019-08-08T12:30:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-08-08</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!--  
            <div id="toc" class="toc-article">
              <strong class="toc-title">文章目录</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Semantic-Segmentation"><span class="toc-number">1.</span> <span class="toc-text">Semantic Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Semantic-Segmentation-Problem"><span class="toc-number">1.1.</span> <span class="toc-text">The Semantic Segmentation Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ConvNets-for-Semantic-Segmentation"><span class="toc-number">1.2.</span> <span class="toc-text">ConvNets for Semantic Segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Semantic-Segmentation-for-Road-Scene-Understanding"><span class="toc-number">1.3.</span> <span class="toc-text">Semantic Segmentation for Road Scene Understanding</span></a></li></ol></li></ol>
            </div>
           -->
        <!--  
    <div id="toc-container"> 
        <div id="toc"> 
            <p> 
                <strong>文章目录</strong>
            </p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Semantic-Segmentation"><span class="toc-text">Semantic Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Semantic-Segmentation-Problem"><span class="toc-text">The Semantic Segmentation Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ConvNets-for-Semantic-Segmentation"><span class="toc-text">ConvNets for Semantic Segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Semantic-Segmentation-for-Road-Scene-Understanding"><span class="toc-text">Semantic Segmentation for Road Scene Understanding</span></a></li></ol></li></ol>
        </div>
    </div>
 -->
        <p>From Coursera, State Estimation and Localization for Self-Driving Cars by University of Toronto<br><a href="https://www.coursera.org/learn/visual-perception-self-driving-cars" target="_blank" rel="noopener">https://www.coursera.org/learn/visual-perception-self-driving-cars</a></p>
<h2 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h2><h3 id="The-Semantic-Segmentation-Problem"><a href="#The-Semantic-Segmentation-Problem" class="headerlink" title="The Semantic Segmentation Problem"></a>The Semantic Segmentation Problem</h3><ul>
<li>Given an input image, we want to classify each pixel into a set of preset categories. The categories can be static road elements e.g. roads, sidewalk, traffic lights or dynamic obstacles e.g. cars, pedestrians, and cyclists or a background class that encompasses any category we do not include in our preset categories.</li>
<li>Semantic segmentation is still through a function estimator<ul>
<li>Given an image, we take every pixel as an input and output a vector of class scores per pixel. </li>
<li>A pixel belongs to the class with the highest class score.</li>
</ul>
</li>
</ul>
<p><strong>Performance of semantic segmentation</strong></p>
<ul>
<li>True Positive(TP): The number of correctly classified pixels belonging to classX</li>
<li>False Positive(FP): The number of pixels that do not belong to class X in ground truth but are classified that class by the algorithm</li>
<li>False Negative(FN): The number of pixels that do belong to class X in ground truth,but are not classified as that class by the algorithm<br>$$IOU_{class} = \frac{TP}{TP+FP+FN}$$</li>
<li>Class lOU over all the data is calculated by computing the sum of TP, FP, FN for all images first</li>
<li>Averaging the class IOU is usually not a very good idea!</li>
<li>CityScapes Segmentation Dataset</li>
</ul>
<h3 id="ConvNets-for-Semantic-Segmentation"><a href="#ConvNets-for-Semantic-Segmentation" class="headerlink" title="ConvNets for Semantic Segmentation"></a>ConvNets for Semantic Segmentation</h3><ul>
<li>Semantic segmentation takes a camera images input and provides a category classification for every pixel in that image as output. This problem can be modeled as a function approximation problem, and ConvNets can once again be used to approximate the required function.</li>
<li>The ConvNet model can be chosen as the same ConvNet model we used for object detection. That is, a feature extractor followed by an output layer. We do not need anchors here as we are not trying to localize objects. </li>
<li>In VGG architecture, <ul>
<li>the input image is the size of MxNx3</li>
<li>As with object detection, the resolution will be reduced by half after every pooling layer. On the other hand, the depth will increase due to the convolutional layers. </li>
<li>The output feature map of the convolutional feature extractor will be downsampled by 16 times.</li>
<li>We can add as many convolutional pooling blocks as needed, but that will further shrink our feature map.</li>
</ul>
</li>
<li>However, output for semantic segmentation is to have a classification for every pixel in the image. How can we achieve this given a down sampled feature map that is 16 times smaller than our original input?</li>
<li>A simple solution would be the following:<ul>
<li>first compute the output by passing the 16 times downsampled output feature map through a softmax layer.</li>
<li>We can then determine the class of every pixel in the subsampled output by taking the highest class score obtained by the softmax layer.</li>
<li>The final step would be to proceed to upsample, the downsampled output back to the original image resolution. </li>
<li>however, this naive upsampling might produce inadequate results</li>
</ul>
</li>
<li>one of the most challenging problems in semantic segmentation is attaining smooth and accurate boundaries around the objects. <ul>
<li>Smooth boundaries are hard to attain as in general boundaries in the image space are ambiguous, especially for thin objects such as pools, similar looking objects such as roads and sidewalks, and far away objects.</li>
</ul>
</li>
<li>Naive upsampling also induces two additional problems:<ul>
<li>Any object less than 16 pixels in width or height usually fully disappears in their upsampled image. This affects both thin objects such as pools and far away objects. As they are below the minimum dimensions required for the pixels to appear from the original image.</li>
</ul>
</li>
<li>To remedy these problems, researchers have formulated what is commonly referred to as a feature decoder. <ul>
<li>The feature decoder can be thought of as a mirror image of the feature extractor. </li>
<li>Instead of using the convolution pooling paradigm to downsample the resolution, it uses upsampling layers followed by a convolutional layer to upsample the resolution of the feature map.</li>
<li>The upsampling usually using nearest neighbor methods achieves the opposite effect to pooling, but results in an inaccurate feature map.</li>
<li>The following convolutional layers are then used to correct the features in the upsampled feature map with learnable filter banks.</li>
<li>This correction usually provides the required smooth boundaries as we go forward through the feature decoder.</li>
</ul>
</li>
<li>Similar to the feature extractor, each upsampling convolution block is referred to as a deconvolution.<ul>
<li>As we go through the first deconvolution block, the input feature map is upsampled to twice the input resolution. </li>
<li>The depth is controlled by how many filters are defined for each successive convolutional layer. </li>
<li>As we go forward through the rest of the decoder, we finally arrive at a feature map of similar resolution to the input image. </li>
</ul>
</li>
<li>The output can be a linear output layer, followed by a softmax function. This layer is very similar to the classification output layer in object detection. <ul>
<li>this layer provides a k-dimensional vector per pixel with the kth element being how confident the neural network is that the pixel belongs to the kth class.</li>
</ul>
</li>
<li>Classification Loss: $L_{cls}=\frac 1{N_{total} \, \sum_i CrossEntropy(s^*_i,s_i)}$<ul>
<li>$N_{total}$ is the total number of classified pixels in all the images of a mini-batch. Usually, a mini-batch would comprise of 8 to 16 images, and the choice of this number depends on how much memory your GPU has.</li>
<li>$s_i$ is the output of the neural network</li>
<li>$s^*_i$ is the ground truth classification</li>
</ul>
</li>
</ul>
<h3 id="Semantic-Segmentation-for-Road-Scene-Understanding"><a href="#Semantic-Segmentation-for-Road-Scene-Understanding" class="headerlink" title="Semantic Segmentation for Road Scene Understanding"></a>Semantic Segmentation for Road Scene Understanding</h3><p><strong>3D Drivable Surface Estimation</strong><br>Steps:</p>
<ol>
<li>Generate semantic segmentation output</li>
<li>Associate 3D point coordinates with 2D image pixels</li>
<li>Choose 3D points belonging to the Drivable Surface category</li>
<li>Estimate 3D drivable surface model</li>
</ol>
<p>Plane Model: $$ax+by+z=d$$<br>Least squares formulation: $$p=[a,b,d], \; argmin_A(Ap-B)$$ $$A = [x_1 \; y_1 \; -1; x_2 \; y_2 \; -1; …; x_N \; y_N \; -1], \quad B = [-z_1; -z_2; … ; -z_N]$$<br>Solution: $p=(A^TA)^{-1}A^TB$</p>
<ul>
<li>Minimum number of points to estimate model: 3 non-collinear points</li>
<li>RANSAC Algorithm:<ul>
<li>From the data, randomly select 3 points.</li>
<li>Compute model parameters a, b, and d using least squares estimation.</li>
<li>Compute number of inliers, N.</li>
<li>If N &gt; threshold, terminate and return the computed plane parameters. Else, go back to step 1.</li>
<li>Recompute the model parameter using all the inliers in the inlier set.</li>
</ul>
</li>
</ul>
<p>Semantic Lane Estimation</p>
<ul>
<li>Estimate the lane, the area where the car can drive on the drivable surface</li>
<li>Estimate what is at the boundaries of the lane:<ul>
<li>Curb</li>
<li>Road</li>
<li>Car</li>
</ul>
</li>
<li>Steps:<ul>
<li>Extract segmentation mask from pixels belonging to lane separators such as lane markings or curbs.</li>
<li>Extract edges from this segmentation mask using an edge detector.</li>
<li>Linear Lane Model:Use the Hough transform to detect lines in the output edge map.</li>
<li>Filter lines based on slope to remove horizontal lines.</li>
<li>Remove any line that does not belong to the drivable space.</li>
<li>Determine which classes occur at the boundary of the lane.</li>
</ul>
</li>
<li>Materials:<ul>
<li>Hough Transform Line Detection: <a href="https://docs.opencv.org/3.4.3/d9/dbo/tutorial_hough_lines.html" target="_blank" rel="noopener">https://docs.opencv.org/3.4.3/d9/dbo/tutorial_hough_lines.html</a></li>
<li>Canny Edge Detection: <a href="https://docs.opencv.org/3.4.3/da/d22/tutorial_py_canny.html" target="_blank" rel="noopener">https://docs.opencv.org/3.4.3/da/d22/tutorial_py_canny.html</a></li>
</ul>
</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Learning</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Self-driving</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2019/08/08/coursera_visual_percep05/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-coursera_visual_percep04" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/coursera_visual_percep04/">Coursera Visual Perception for Self-Driving Cars 04</a>
    </h1>
  

        
        <a href="/2019/08/08/coursera_visual_percep04/" class="archive-article-date">
  	<time datetime="2019-08-08T02:30:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-08-08</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!--  
            <div id="toc" class="toc-article">
              <strong class="toc-title">文章目录</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#2D-Object-Detection"><span class="toc-number">1.</span> <span class="toc-text">2D Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Object-Detection-Problem"><span class="toc-number">1.1.</span> <span class="toc-text">The Object Detection Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2D-Object-detection-with-Convolutional-Neural-Networks"><span class="toc-number">1.2.</span> <span class="toc-text">2D Object detection with Convolutional Neural Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-vs-Inference"><span class="toc-number">1.3.</span> <span class="toc-text">Training vs. Inference</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-2D-Object-Detectors-for-Self-Driving-Cars"><span class="toc-number">1.4.</span> <span class="toc-text">Using 2D Object Detectors for Self-Driving Cars</span></a></li></ol></li></ol>
            </div>
           -->
        <!--  
    <div id="toc-container"> 
        <div id="toc"> 
            <p> 
                <strong>文章目录</strong>
            </p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#2D-Object-Detection"><span class="toc-text">2D Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Object-Detection-Problem"><span class="toc-text">The Object Detection Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2D-Object-detection-with-Convolutional-Neural-Networks"><span class="toc-text">2D Object detection with Convolutional Neural Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-vs-Inference"><span class="toc-text">Training vs. Inference</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-2D-Object-Detectors-for-Self-Driving-Cars"><span class="toc-text">Using 2D Object Detectors for Self-Driving Cars</span></a></li></ol></li></ol>
        </div>
    </div>
 -->
        <p>From Coursera, State Estimation and Localization for Self-Driving Cars by University of Toronto<br><a href="https://www.coursera.org/learn/visual-perception-self-driving-cars" target="_blank" rel="noopener">https://www.coursera.org/learn/visual-perception-self-driving-cars</a></p>
<h2 id="2D-Object-Detection"><a href="#2D-Object-Detection" class="headerlink" title="2D Object Detection"></a>2D Object Detection</h2><h3 id="The-Object-Detection-Problem"><a href="#The-Object-Detection-Problem" class="headerlink" title="The Object Detection Problem"></a>The Object Detection Problem</h3><p><strong>2D object detection task problem</strong></p>
<ul>
<li>Object detection can be defined as a function estimation problem</li>
<li>Given an input image x, we want to find the function $f(x;\theta)$ that produces an output vector that includes the coordinates of the top-left of the box, and the coordinates of the lower right corner of the box, and a class score: $f(x;\theta) = [x_{min}, y_{min}, x_{max}, y_{max}, S_{class_1},…S_{class_k}]$</li>
</ul>
<p><strong>Evaluating performance measures</strong></p>
<ul>
<li>The first step of the evaluation process is to compare the detector localization output to the ground truth boxes via the Intersection-Over-Union metric (IOU).<ul>
<li>area of intersection of predicted box with a ground truth box, divided by the area of their union</li>
</ul>
</li>
<li>To account for class scores, we define true positives (TP),  False positive (FP) and False Negative (FN). <ul>
<li>TP: Object class score &gt; score threshold, and IOU &gt; IOU threshold</li>
<li>FP: Object class score &gt; score threshold, and IOU &lt; IOU threshold</li>
<li>FN: Number of ground truth objects not detected by the algorithm</li>
<li>Precision: TP/(TP+FP)</li>
<li>Recall: TP/(TP+FN)</li>
<li>Precision Recall Curve (PR-Curve): Use multiple object class score thresholds to compute precision and recall. Plot the values with precision on y-axis, and recall on x-axis</li>
<li>Average Precision (AP): Area under PR-Curve for a single class. Usually approximated using 11 recall points</li>
</ul>
</li>
</ul>
<h3 id="2D-Object-detection-with-Convolutional-Neural-Networks"><a href="#2D-Object-detection-with-Convolutional-Neural-Networks" class="headerlink" title="2D Object detection with Convolutional Neural Networks"></a>2D Object detection with Convolutional Neural Networks</h3><p>The Feature Extractor</p>
<ul>
<li>Feature extractors are the most computationally expensive component of the 2D object detector</li>
<li>The output of feature extractors usually has much lower width and height than those of the input image, but much greater depth</li>
<li>Very active area of research, with new extractors proposed on regular basis</li>
<li>Most common extractors are: VGG, ResNet, and Inception</li>
</ul>
<p>VGG Feature Extractor</p>
<ul>
<li>Alternating convolutional and pooling layers</li>
<li>All convolutional layers are of size 3×3xK, with stride 1 and 1 zero-padding</li>
<li>All pooling layers use the max function, and are of size 2x2, with stride 2 and no padding.</li>
</ul>
<p>Prior boxes/anchor boxes:</p>
<ul>
<li>To generate 2D bounding boxes, we usually do not start from scratch and estimate the corners of the bounding box without any prior. </li>
<li>We assume that we do have a prior on where the boxes are in image space and how large these boxes should be. These priors are called anchor boxes and are manually defined over the whole image usually on an equally-spaced grid. </li>
<li>During training, the network learns to take each of these anchors and tries to move it as close as possible to the ground truth bounding box in both the centroid location and box dimensions. This is termed <em>residual learning</em> and it takes advantage of the notion that it is easier to nudge an existing box a small amount to improve it rather than to search the entire image for possible object locations.</li>
<li>Residual learning has proven to provide much better results than attempting to directly estimate bounding boxes without any prior. </li>
</ul>
<p>Faster R-CNN:</p>
<ul>
<li>For every pixel in the feature map, we associate k anchor boxes. </li>
<li>We then perform a 3x3xD star convolution operation on that pixels neighborhood. This results in a 1x1xD star feature vector for that pixel. </li>
<li>We use this one by 1x1xD star feature vector as the feature vector of every one of the k anchors associated with that pixel. </li>
<li><p>We then proceed to feed the extracted feature vector to the output layers in the neural network.</p>
</li>
<li><p>The output layers of a 2D object detector usually comprise of a regression head and a classification head. </p>
</li>
<li>The regression head usually includes multiple fully-connected hidden layers with a linear output layer. The regressed output is typically a vector of residuals that need to be added to the anchor that hand to get the ground truth bounding box.</li>
<li>Another method to update the dimension of the anchors is to regress a residual from the center of the anchor to the center of the ground truth bounding box in addition to two scale factors that correct the ground truth bounding box width and height when multiplied with an anchor’s width and height. </li>
<li>The classification head is also comprised of multiple fully-connected hidden layers, but with a final softmax output layer. The softmax output is a vector with a single score per class. The highest score usually defines the class of the anchor at hand.</li>
</ul>
<h3 id="Training-vs-Inference"><a href="#Training-vs-Inference" class="headerlink" title="Training vs. Inference"></a>Training vs. Inference</h3><p><strong>Minibatch selection</strong></p>
<ul>
<li>Negative anchors target:<ul>
<li>Classification:Background</li>
<li>Regression:None</li>
</ul>
</li>
<li>Positive anchors target:<ul>
<li>Classification:Category of the ground truth bounding box</li>
<li>Regression:Align box parameters with highest IOU ground truth bounding box</li>
</ul>
</li>
<li>Problem: Majority of anchors are negatives results in neural network will label all detections as background</li>
<li>Solution: Sample a chosen minibatch size,with 3:1 ratio of negative to positive anchors to eliminate bias towards the negative class</li>
<li>Choose negatives with highest classification loss(online hard negative mining) to be included in the minibatch</li>
<li>Classification loss: $L_{cls} = \frac 1{N_{total}} \sum_i crossentropy(s^*_i, s_i)$<ul>
<li>$N_{total}$ is the size of the minibatch</li>
<li>$s_i$ is the output of the nerual network</li>
<li>$s^*_i$ is the anchor classification target:<ul>
<li>Background if anchor is negative</li>
<li>Ground truth box class if anchor is positive</li>
</ul>
</li>
</ul>
</li>
<li>Regression Loss: $L_{reg} = \frac 1{N_{total}} \sum_i p_iL_2(b^*_i, b_i) $<ul>
<li>$p_i$ is 0 if anchor is negative and 1 if anchor is positive</li>
<li>$N_p$ is the number of positive anchors in the minibatch</li>
<li>$b^*_i$ the ground truth bounding box</li>
<li>$b_i$ is the estimated bounding box, applying the regressed residuals to the anchor box parameters</li>
</ul>
</li>
</ul>
<p><strong>Non-maximum suppression(NMS)</strong></p>
<ul>
<li>an extremely powerful approach to improving inference output for anchor based neuron networks. </li>
<li>Non-max suppression takes as an input a list of predicted boundary boxed b, and each bounding blocks is comprised of the regressed coordinates in the class output score.</li>
<li>It also needs as an input a predefined IOU threshold which we’ll call ada.</li>
<li>Algorithm goes as follows<ul>
<li>first sort the bounding boxes in list B according to their output score. We also initialize an empty set D to hold output bounding boxes.</li>
<li>then proceed to iterate overall elements in the sorted box list B bar. Inside the for loop, we first determine the box B max with the highest score in the list B, which should be the first element in B bar.</li>
<li>then remove this bounding box from the bounding box set D bar and add it to the output set D.</li>
<li>Next, find all boxes remaining in the set B bar that have an IOU greater than ada with the box B max. These boxes significantly overlap with the current maximum box, B max. Any box that satisfies this condition gets removed from the list B bar. We keep iterating through the list B bar until is empty, and then we return the list D.</li>
<li>D now contains a single bounding box per object.</li>
</ul>
</li>
</ul>
<h3 id="Using-2D-Object-Detectors-for-Self-Driving-Cars"><a href="#Using-2D-Object-Detectors-for-Self-Driving-Cars" class="headerlink" title="Using 2D Object Detectors for Self-Driving Cars"></a>Using 2D Object Detectors for Self-Driving Cars</h3><p><strong>3D Object Detection</strong></p>
<ul>
<li>Estimating the: <ul>
<li>Category Classification: Car, pedestrian, cyclist </li>
<li>Position of the centroid in 3D: $[x,y,z]$</li>
<li>Extent in 3D: $[l,w,h]$ </li>
<li>Orientation in 3D Y: $[\phi,\psi,\theta]$</li>
</ul>
</li>
<li>The most common and successful way to extend 2D object detection results in 3D is to use LiDAR point clouds.<ul>
<li>Given a 2D bounding box in an image space and a 3D LiDAR point cloud, we can use the inverse of the camera projection matrix to project the corners of the bounding box as rays into the 3D space. </li>
</ul>
</li>
<li>Advantages:<ul>
<li>Allows exploitation of mature 2D object detectors, with high precision and recall </li>
<li>Class already determined from 2D detection. There is no need to use LiDAR data or pass 3D information into the network to determine whether we are looking at a car or a post.</li>
<li>Does not require prior scene knowledge,such as ground plane location</li>
</ul>
</li>
<li>Disadvantages:<ul>
<li>The performance of the 3D estimator is bounded by the performance of the 2D detector</li>
<li>Occlusion and truncation are hard to handle from 2D only</li>
<li>3D estimator needs to wait for 2D detector, inducing latency in our system</li>
</ul>
</li>
</ul>
<p><strong>2D object tracking</strong></p>
<ul>
<li>Detection: We detect the object independently ineach frame and can record its position over time</li>
<li>Tracking: We use image measurements to estimate position of object, but also incorporate position predicted by dynamics, i.e., our expectation of object’s motion pattern</li>
<li>Tracking Assumptions: <ul>
<li>Camera is not moving instantly to new viewpoint </li>
<li>Objects do not disappear and reappear in different places in the scene </li>
<li>If the camera is moving,there is a gradual change in pose between camera and scene</li>
</ul>
</li>
<li>Object Tracking: Prediction<ul>
<li>Each object will have a predefined motion model in image space, e.g. $p_k = p_{k-1} +v_k\Delta t + N(0,\Sigma)$</li>
</ul>
</li>
<li>Object Tracking: correlation<ul>
<li>Get Measurement Bounding Boxes from 2D detector.</li>
<li>Correlate prediction with the highest IOU measurement</li>
</ul>
</li>
<li>Object Tracking: Update<ul>
<li>The prediction and measurement are fused as part of the Kalman Filter Framework</li>
</ul>
</li>
<li>For each frame, we start new track if a measurement has no correlated prediction</li>
<li>We also terminate inconsistent tracks, if a predicted object does not correlate with a measurement for a preset number of frames</li>
<li>The same methodology can be used to track objects in 3D!</li>
</ul>
<p><strong>Traffic signs and signals detection</strong></p>
<ul>
<li>Traffic signs and signals appear smaller in size compared to cars, two-wheelers, and pedestrians.</li>
<li>Traffic signs are highly variable with many classes to be trained on.</li>
<li>Traffic signals have different states that are required to be detected.</li>
<li>In addition, traffic signals change state as the car drives</li>
<li>2D object detectors can be used to perform traffic sign and traffic signal detection without any modifications</li>
<li>However, multi-stage hierarchical models have been shown to outperform the standard single stage object detectors Prior Boxes</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Learning</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Self-driving</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2019/08/08/coursera_visual_percep04/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-coursera_visual_percep03" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/07/coursera_visual_percep03/">Coursera Visual Perception for Self-Driving Cars 03</a>
    </h1>
  

        
        <a href="/2019/08/07/coursera_visual_percep03/" class="archive-article-date">
  	<time datetime="2019-08-07T13:30:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-08-07</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!--  
            <div id="toc" class="toc-article">
              <strong class="toc-title">文章目录</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Feedforward-Neural-Networks"><span class="toc-number">1.</span> <span class="toc-text">Feedforward Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Feed-Forward-Neural-Networks"><span class="toc-number">1.1.</span> <span class="toc-text">Feed Forward Neural Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Output-Layers-and-Loss-Functions"><span class="toc-number">1.2.</span> <span class="toc-text">Output Layers and Loss Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-Network-Training-with-Gradient-Descent"><span class="toc-number">1.3.</span> <span class="toc-text">Neural Network Training with Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Splits-and-Neural-Network-Performance-Evaluation"><span class="toc-number">1.4.</span> <span class="toc-text">Data Splits and Neural Network Performance Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-Network-Regularization"><span class="toc-number">1.5.</span> <span class="toc-text">Neural Network Regularization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolutional-Neural-Networks"><span class="toc-number">1.6.</span> <span class="toc-text">Convolutional Neural Networks</span></a></li></ol></li></ol>
            </div>
           -->
        <!--  
    <div id="toc-container"> 
        <div id="toc"> 
            <p> 
                <strong>文章目录</strong>
            </p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Feedforward-Neural-Networks"><span class="toc-text">Feedforward Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Feed-Forward-Neural-Networks"><span class="toc-text">Feed Forward Neural Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Output-Layers-and-Loss-Functions"><span class="toc-text">Output Layers and Loss Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-Network-Training-with-Gradient-Descent"><span class="toc-text">Neural Network Training with Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Splits-and-Neural-Network-Performance-Evaluation"><span class="toc-text">Data Splits and Neural Network Performance Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-Network-Regularization"><span class="toc-text">Neural Network Regularization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolutional-Neural-Networks"><span class="toc-text">Convolutional Neural Networks</span></a></li></ol></li></ol>
        </div>
    </div>
 -->
        <p>From Coursera, State Estimation and Localization for Self-Driving Cars by University of Toronto<br><a href="https://www.coursera.org/learn/visual-perception-self-driving-cars" target="_blank" rel="noopener">https://www.coursera.org/learn/visual-perception-self-driving-cars</a></p>
<h2 id="Feedforward-Neural-Networks"><a href="#Feedforward-Neural-Networks" class="headerlink" title="Feedforward Neural Networks"></a>Feedforward Neural Networks</h2><h3 id="Feed-Forward-Neural-Networks"><a href="#Feed-Forward-Neural-Networks" class="headerlink" title="Feed Forward Neural Networks"></a>Feed Forward Neural Networks</h3><p>A Feedforward Neural Network defines a mapping from input x to output y as: $y=f(x;0)$</p>
<ul>
<li>We define:<ul>
<li>$x$ is called the input layer</li>
<li>The final function $f^{(N)}$ is called the output layer</li>
<li>The functions $f^{(1)}$ to $f^{(N-1)}$ are called the hidden layers</li>
</ul>
</li>
<li>Functions to estimate:<ul>
<li>Object Classification: Image → Label</li>
<li>Object Detection: Image → Label+Location</li>
<li>Depth Estimation: Image → Depth for every pixel</li>
<li>Semantic Segmentation: Image → Label for every pixel</li>
</ul>
</li>
<li>Mode Of Action Of Neural Networks<ul>
<li>Training: Give neural network examples of $f^*(x)$.<br>for a wide variation of the input $x$. Then, optimize its parameters $0$ to force $f(x;0) \approx f^\ast(x)$</li>
<li>Pairs of $x$ and $f^*(x)$ are called training data</li>
<li>Only output is specified by training data! Network is free to do anything with its hidden layers</li>
</ul>
</li>
<li>Hidden Units: $h_n = g(W^Th_{n-1} + b)$<ul>
<li>Activation function $g$</li>
<li>Input $h_{n-1}$</li>
<li>Weight matrix $W$</li>
<li>Bias $b$</li>
<li>Parameters $\theta$ are the weights and biases of all the layers of the network</li>
<li>Transformed parameters passed through activation function $g$</li>
</ul>
</li>
<li>The Rectified Linear Unit: ReLU<ul>
<li>The ReLU hidden unit is currently the default choice of activation function for Feedforward Neural Networks $g(z)=max(0, z)$ </li>
</ul>
</li>
</ul>
<h3 id="Output-Layers-and-Loss-Functions"><a href="#Output-Layers-and-Loss-Functions" class="headerlink" title="Output Layers and Loss Functions"></a>Output Layers and Loss Functions</h3><p><strong>General process of designing machine learning algorithm</strong></p>
<ul>
<li>Inference:<ul>
<li>a feed-forward neural network takes an input $x$, passes it through a sequence of hidden layers, then passes the output of the hidden layers through an output layer.</li>
</ul>
</li>
<li>Training:<ul>
<li>pass the predicted output through the loss function, then use an optimization procedure to produce a new set of parameters data that provide a lower value for the loss function.</li>
</ul>
</li>
<li>Tasks in self-driving: Classification and Regression<ul>
<li>Classification: Given input x map it to one of k classes or categories.<ul>
<li>Image classification, semantic segmentation</li>
</ul>
</li>
<li>Regression: Given input x map it to a real number:<ul>
<li>Depth prediction, bounding box estimation</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Loss functions in different tasks</strong></p>
<ul>
<li>Classification: Softmax Output Layers<ul>
<li>Softmax output layers are most often used as the output of a classifier, to represent the probability distribution over K different classes</li>
<li>The Softmax output layer is comprised of:<ul>
<li>A linear transformation: $z=W^Th+b$</li>
</ul>
</li>
<li>Followed by the Softmax function: $$Softmax(z_i)= \frac{exp(z_i)}{\sum_j exp(z_j)}$$</li>
</ul>
</li>
<li>Classification: Cross-Entropy Loss Function<ul>
<li>By considering the output of the softmax output layer as aprobability distribution, the Cross Entropy Loss function is derived using maximum likelihood as: $$L(\theta) = -log(softmax(z_i)) = -z_i + log\sum_j exp(z_j) $$</li>
<li>The Cross-Entropy Loss has two terms to control how close the output of the network is to the true probability:<ul>
<li>$Z_i$ is the output of the hidden layer corresponding to the true class before being passed through the softmax function. This is usually called the class logit which comes from the field of logistic regression. The negative of the class logit $z_i$ encourages the network to output a large value for the probability of the correct class. </li>
<li>The second term on the other hand, encourages the output of the affine transformation to be small</li>
</ul>
</li>
</ul>
</li>
<li>Regression: Linear Output Layers<ul>
<li>Linear Output Units are based only on an affine transformation with no non-linearity: $z=W^Th+b$</li>
<li>Linear Output Units are usually used with the Mean Squared Error loss function to model the mean of a probability distribution: $$L(\theta) = \sum_i (z_i - f^*(x_i))^2$$</li>
</ul>
</li>
</ul>
<h3 id="Neural-Network-Training-with-Gradient-Descent"><a href="#Neural-Network-Training-with-Gradient-Descent" class="headerlink" title="Neural Network Training with Gradient Descent"></a>Neural Network Training with Gradient Descent</h3><p>Neural Network Loss Functions</p>
<ul>
<li>Thousands of training example pairs $[x,f^*(x)]$</li>
<li>The Loss function computed over all $N$ training examples is termed the Training Loss and can be written as: $J(\theta) = \frac 1N \sum^N_{i=1} L[f(x_i,\theta), f^*(x_i)]$</li>
<li>The gradient of the training loss with respect to the parameters $\theta$ can be written as: $$\nabla_\theta J(\theta) = \nabla_\theta [\frac 1N \sum^N_{i=1} L[f(x_i, \theta), f^*(x_i)]] = \frac 1N \sum^N_{i=1} \nabla_\theta L[f(x_i, \theta), f^\ast(x_i)] $$</li>
</ul>
<p>Batch Gradient Descent:</p>
<ul>
<li>Batch Gradient Descent is an iterative first order optimization procedure</li>
<li>Iterative means that it starts from an initial guess of parameters theta and improves on these parameters iteratively. </li>
<li>First order means that the algorithm only uses the first order derivative to improve the parameters theta. </li>
<li>Batch Gradient Descent Algorithm: <ul>
<li>Initialize parameters $\theta$</li>
<li>While Stopping Condition is Not Met:<ul>
<li>Compute gradient of loss function over all training examples using the above training loss</li>
<li>Update parameters according to: $\theta \leftarrow \theta - \epsilon \nabla_\theta J(\theta)$</li>
<li>$\epsilon$ is called the learning rate and controls how much we adjust the parameters in the direction of the negative gradient at every iteration.</li>
</ul>
</li>
</ul>
</li>
<li>Backpropagation used to compute $\nabla_{\theta}J(\theta)$ is very expensive to compute over the whole training dataset.<ul>
<li>Luckily, the lose function as well as its gradient are means over the training dataset</li>
<li>Standard error of the mean estimated from N samples is $\frac {\sigma}{\sqrt N}$, where $\sigma$ is the standard deviation of the value of the samples</li>
<li>Using all samples to estimate the gradient results in less than linear return in accuracy of this estimate</li>
<li>Use a small subsample (Minibatch) of the training data to estimate the gradient</li>
</ul>
</li>
<li>The Stochastic (minibatch) Gradient Descent just alterate at the sampling step.</li>
<li>Choose of Minibatch Size: <ul>
<li>GPUs work better with powers of 2 batch sizes</li>
<li>Large batch sizes &gt; 256: <ul>
<li>Hardware underutilized with very small batch sizes.</li>
<li>More accurate estimate of the gradient, but with less than linear returns</li>
</ul>
</li>
<li>Small batch size &lt; 64<ul>
<li>Small batches can offer a regularizing effect.The best generalization error is often achieved with batch size of 1.</li>
<li>Small batch sizes allow for faster convergence, as the algorithm can compute the parameter updates rapidly</li>
</ul>
</li>
<li>As a result of these trade-offs, typical power of two mini batch sizes range from 32 to 256, with smaller sizes sometimes being attempted for large models or to improve generalization.</li>
<li>Always make sure dataset is shuffled before sampling minibatch</li>
</ul>
</li>
</ul>
<p>Parameter Initialization and Stopping Conditions</p>
<ul>
<li>Parameter Initialization: <ul>
<li>Weights: initialized by randomly sampling from a standard normal distribution </li>
<li>Biases: initialized to 0</li>
<li>Other heuristics exist</li>
</ul>
</li>
<li>Stopping Conditions:<ul>
<li>Number of iterations: How many training iterations the neural network has performed</li>
<li>Change $ln \theta$ value: Stop if $\theta_{new} - \theta_{old}$ &lt; Threshold</li>
<li>Change $ln J(\theta)$ value: Stop if $J(\theta_{new})-J(\theta_{old})$ &lt; Threshold</li>
</ul>
</li>
</ul>
<p>SGD Variations</p>
<ul>
<li>Many variations of SGD exist<ul>
<li>Momentum SGD, Nestrove Momentum SGD </li>
<li>Ada-Grad, RMS-Prop </li>
<li>ADAM (Adaptive Moment Estimation)</li>
</ul>
</li>
<li>Which one to use?<ul>
<li>ADAM: Implemented in most deep neural network libraries, fairly robust to the choice of the learning rate and other hyperparameters</li>
</ul>
</li>
</ul>
<h3 id="Data-Splits-and-Neural-Network-Performance-Evaluation"><a href="#Data-Splits-and-Neural-Network-Performance-Evaluation" class="headerlink" title="Data Splits and Neural Network Performance Evaluation"></a>Data Splits and Neural Network Performance Evaluation</h3><ul>
<li>Data splits:<ul>
<li>training: used to minimize the Loss Function</li>
<li>validation: used to choose best hyperparameters, such as the learning rate, number of layers, etc.</li>
<li>testing: the neural network never observes this set. The developer never uses this set in the design process</li>
</ul>
</li>
<li>Percentage of split:<ul>
<li>total sample size is ~10000: training 60%, validation 20%, testing 20%</li>
<li>total sample size is ~1000000: training 98%, validation 1%, testing 1%</li>
</ul>
</li>
<li>Behavior of Split Specific Loss Functions:<ul>
<li>overfitting, underfitting</li>
<li>The gap between training and validation loss is called the generalization gap</li>
</ul>
</li>
<li>Reducing the Effect of Underfitting/Overfitting<ul>
<li>Underfitting: (Training loss is high)<ul>
<li>Train longer</li>
<li>More layers or more parameters per layer</li>
<li>Change architecture</li>
</ul>
</li>
<li>Overfitting: (Generalization gap is large)<ul>
<li>More training data</li>
<li>Regularization</li>
<li>Change architecture</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Neural-Network-Regularization"><a href="#Neural-Network-Regularization" class="headerlink" title="Neural Network Regularization"></a>Neural Network Regularization</h3><p>Remedy overfitting through various regularization strategies:</p>
<ul>
<li>Parameter norm penalties<ul>
<li>$J(\theta)_{reg} = J(\theta) + \alpha \Omega(\theta)$ </li>
<li>limits the capacity of the model by adding the penalty $\omega$ of $\theta$ to the objective function. </li>
<li>$\alpha$ is a hyperparameter that weights the relative contribution of the norm penalty to the value of the loss function</li>
<li>$\Omega(\theta)$ is a measure of how large $\theta$’s value is, usually an $L_p$ Norm. </li>
<li>We usually only constrain the size of weights and not biases: $J(\theta)_{reg} = J(\theta) + \alpha \Omega(W)$ </li>
<li>The most common norm penalty used in neural networks is the L2-norm penalty: $\Omega(W) = \frac 12 W^TW = \frac 12 |W|^2_2$</li>
</ul>
</li>
<li>Dropout<ul>
<li>The first step of dropout is to choose a probability which we’ll call $P_{keep}$.</li>
<li>At every training iteration, this probability is used to choose a subset of the network nodes to keep in the network. These nodes can be either hidden units, output units, or input units.</li>
<li>We then proceed to evaluate the output y after cutting all the connections coming out of this unit. </li>
<li>Since we are removing units proportional to the keep probably, $P_{keep}$, we multiply the final weights by $P_{keep}$ at the ending of training. This is essential to avoid incorrectly scaling the outputs when we switch to inference for the full network.</li>
<li>Computationally inexpensive but powerful regularization method</li>
<li>Does not significantly limit the type of model or training procedure that can be used. Works well with nearly any model that uses a distributed over parameterized representation, and that can be trained with stochastic gradient descent</li>
<li>Dropout layers are practically implemented in all neural network libraries.</li>
</ul>
</li>
<li>Early Stopping<ul>
<li>Early stopping ends training when the validation loss keeps increasing for a preset number of iterations or epochs.</li>
<li>Early stopping should not be use as a first choice for regularization. As it also limits the training time, which may interfere with the overall network performance.</li>
</ul>
</li>
</ul>
<h3 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h3><p><strong>ConvNets</strong></p>
<ul>
<li>Used for processing data defined on grid</li>
<li>1Dtime series data,2D images,3D videos</li>
<li>Two major type of layers:<ul>
<li>Convolution Layers</li>
<li>Pooling Layers</li>
</ul>
</li>
<li>Cross-correlation:<ul>
<li>The vertical and horizontal shifts are usually the same value, referred as the stride of our convolutional layer. </li>
</ul>
</li>
<li>Output Volume Shape<ul>
<li>Filters are size mxm</li>
<li>Number of filters = K</li>
<li>Stride = S, Padding = P</li>
<li>the expression for width: $W_{out} = \frac{W_{in}-m+2*P}{S} +1$</li>
<li>the expression for height: $H_{out} = \frac{H_{in}-m+2*P}{S} +1$</li>
<li>the expression for depth: $D_{out} = K$</li>
</ul>
</li>
</ul>
<p><strong>Pooling</strong></p>
<ul>
<li>A pooling layer uses pooling functions to replace the output of the previous layer with a summary statistic of the nearby outputs. </li>
<li>Pooling helps make the representations become invariant to small translations of the input. </li>
<li>Max pooling:<ul>
<li>Max pooling summarizes output volume patches with the max function.</li>
<li>Output Volume Shape<ul>
<li>Pool size nxn</li>
<li>Stride = S</li>
<li>$W_{out} = \frac{W_{in}-n}{S} +1$</li>
<li>$H_{out} = \frac{H_{in}-n}{S} +1$</li>
<li>$D_{out} = D_{in}$</li>
</ul>
</li>
</ul>
</li>
<li>Advantages of ConvNets<ul>
<li>Convolutional neural networks are by design, a natural choice to process images</li>
<li>Convolutional layers have <em>less parameters</em> than fully connected layers, reducing the chances of overfitting</li>
<li>Convolutional layers use the same parameters to process every block of the image. Along with pooling layers, this leads to <em>translation invariance</em>, which is particularly important for image understanding</li>
</ul>
</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Learning</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Self-driving</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2019/08/07/coursera_visual_percep03/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-coursera_visual_percep02" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/06/coursera_visual_percep02/">Coursera Visual Perception for Self-Driving Cars 02</a>
    </h1>
  

        
        <a href="/2019/08/06/coursera_visual_percep02/" class="archive-article-date">
  	<time datetime="2019-08-06T13:30:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-08-06</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!--  
            <div id="toc" class="toc-article">
              <strong class="toc-title">文章目录</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Visual-Features-Detection-Description-and-Matching"><span class="toc-number">1.</span> <span class="toc-text">Visual Features - Detection, Description and Matching</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction-to-Image-features-and-Feature-Detectors"><span class="toc-number">1.1.</span> <span class="toc-text">Introduction to Image features and Feature Detectors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction-to-feature-detecors"><span class="toc-number">1.2.</span> <span class="toc-text">Introduction to feature detecors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-Descriptors"><span class="toc-number">1.3.</span> <span class="toc-text">Feature Descriptors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-Matching"><span class="toc-number">1.4.</span> <span class="toc-text">Feature Matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-Matching-Handling-Ambiguity-in-Matching"><span class="toc-number">1.5.</span> <span class="toc-text">Feature Matching: Handling Ambiguity in Matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Outlier-Rejection"><span class="toc-number">1.6.</span> <span class="toc-text">Outlier Rejection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Visual-Odometry"><span class="toc-number">1.7.</span> <span class="toc-text">Visual Odometry</span></a></li></ol></li></ol>
            </div>
           -->
        <!--  
    <div id="toc-container"> 
        <div id="toc"> 
            <p> 
                <strong>文章目录</strong>
            </p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Visual-Features-Detection-Description-and-Matching"><span class="toc-text">Visual Features - Detection, Description and Matching</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction-to-Image-features-and-Feature-Detectors"><span class="toc-text">Introduction to Image features and Feature Detectors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction-to-feature-detecors"><span class="toc-text">Introduction to feature detecors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-Descriptors"><span class="toc-text">Feature Descriptors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-Matching"><span class="toc-text">Feature Matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-Matching-Handling-Ambiguity-in-Matching"><span class="toc-text">Feature Matching: Handling Ambiguity in Matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Outlier-Rejection"><span class="toc-text">Outlier Rejection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Visual-Odometry"><span class="toc-text">Visual Odometry</span></a></li></ol></li></ol>
        </div>
    </div>
 -->
        <p>From Coursera, State Estimation and Localization for Self-Driving Cars by University of Toronto<br><a href="https://www.coursera.org/learn/visual-perception-self-driving-cars" target="_blank" rel="noopener">https://www.coursera.org/learn/visual-perception-self-driving-cars</a></p>
<h2 id="Visual-Features-Detection-Description-and-Matching"><a href="#Visual-Features-Detection-Description-and-Matching" class="headerlink" title="Visual Features - Detection, Description and Matching"></a>Visual Features - Detection, Description and Matching</h2><h3 id="Introduction-to-Image-features-and-Feature-Detectors"><a href="#Introduction-to-Image-features-and-Feature-Detectors" class="headerlink" title="Introduction to Image features and Feature Detectors"></a>Introduction to Image features and Feature Detectors</h3><h3 id="Introduction-to-feature-detecors"><a href="#Introduction-to-feature-detecors" class="headerlink" title="Introduction to feature detecors"></a>Introduction to feature detecors</h3><p><strong>Feature extraction</strong></p>
<ul>
<li>Application: image stitching<ul>
<li>given two images from two different cameras, we would like to stitch them together to form a panorama. </li>
<li>First, we need to identify distinctive points in our images. We call this point image features. </li>
<li>Second, we associate a descriptor for each feature from its neighborhood. </li>
<li>Finally, we use these descriptors to match features across two or more images. </li>
</ul>
</li>
<li>Features are points of interest in an image</li>
<li>Points of interest should have the following characteristics: <ul>
<li>Saliency: distinctive, identifiable,and different from its immediate neighborhood </li>
<li>Repeatability: can be found in multiple images using same operations </li>
<li>Locality: occupies a relatively small subset of image space </li>
<li>Quantity: enough points represented In the image</li>
<li>Efficiency: reasonable computation time</li>
</ul>
</li>
<li>How to choose the points of interest:<ul>
<li>Repetitive texture less patches are challenging to detect consistently</li>
<li>Patches with large contrast changes (gradients) are easier to detect(edges)</li>
<li>Gradients in at least two(significantly) different orientations are the easiest to detect(corners)</li>
<li>The most famous corner detector is the Harris Corner Detector, which uses image gradient information to identify pixels that have a strong change in intensity in both x and y directions.</li>
</ul>
</li>
</ul>
<p><strong>Algorithms used</strong></p>
<ul>
<li>Harris[corners]: Easy to compute, but not scale invariant.<ul>
<li>meaning that the corners can look different depending on the distance the camera is away from the object generating the corner. </li>
</ul>
</li>
<li>Harris-Laplace[corners]: Same procedure as Harris detector, addition of scale selection based on Laplacian. Scale invariance.</li>
<li>Features from accelerated segment test(FAST) (corners): Machine learning approach for fast corner detection.<ul>
<li>high computational efficiency and solid detection performance</li>
</ul>
</li>
<li>Laplacian of Gaussian(LOG) detector[blobs]: Uses the concept of scale space in a large neighborhood(blob). Somewhat scale invariant.</li>
<li>Difference of Gaussian(DOG) detector[blobs]: Approximates LOG but is faster to compute</li>
</ul>
<h3 id="Feature-Descriptors"><a href="#Feature-Descriptors" class="headerlink" title="Feature Descriptors"></a>Feature Descriptors</h3><p><strong>Feature descriptor definition</strong></p>
<ul>
<li>Feature: Point of interest in an image defined by its image pixel coordinates [u,v]</li>
<li>Descriptor f: an n dimensional vector associated with each feature.<ul>
<li>The descriptor has the task of providing a summary of the image information in the vicinity of the feature itself, and can take on many forms. </li>
</ul>
</li>
<li>Feature descriptors should have the following characteristics:      <ul>
<li>Repeatability: manifested as robustness and invariance to translation, rotation,scale, and illumination changes</li>
<li>Distinctiveness: should allow us to distinguish between two close by features,very important for matching later on</li>
<li>Compactness &amp; Efficiency: reasonable computation time</li>
</ul>
</li>
<li>Designing Invariant Descriptors: SIFT<ul>
<li>Scale Invariant Feature Transform (SIFT)descriptors</li>
<li>Given a feature in the image, the shift descriptor takes a 16 by 16 window of pixels around it, we call this window the features local neighborhood.</li>
<li>then separate this window in to four 4 by 4 cells such that each cell contains 16 pixels.</li>
<li>Next we compute the edges and edge orientation of each pixel in each cell using the gradient filters.</li>
<li>For stability of the descriptor, we suppress weak edges using a predefined threshold as they are likely to vary significantly in orientation with small amounts of noise between images.</li>
<li>Finally, we compute a 32 dimensional histogram of orientations for each cell. And concatenate the histograms for all four cells to get a final 128 dimensional histogram for the feature at hand, we call this histogram or descriptor. </li>
</ul>
</li>
<li>Scale Invariant Feature Transform<ul>
<li>SIFT is an example of a very well human engineered feature descriptor, and is used in many state-of-the-art systems</li>
<li>The above process is usually compute on rotatedand scaled version of the 16x16 window, allowing for better scale robustness</li>
<li>Combined with the DOG feature detector, SIFT descriptors provide a scale,rotation, and illumination invariant detector/descriptor pair.</li>
</ul>
</li>
</ul>
<p><strong>Algorithms</strong><br>Other Descriptors:</p>
<ul>
<li>Speeded-Up Robust Features (SURF)</li>
<li>Gradient Location-Orientation Histogram (GLOH)</li>
<li>Binary Robust Independent Elementary Features (BRIEF)</li>
<li>Oriented Fast and Rotated Brief (ORB): free to use commertially</li>
<li>Many more!</li>
</ul>
<h3 id="Feature-Matching"><a href="#Feature-Matching" class="headerlink" title="Feature Matching"></a>Feature Matching</h3><p><strong>Match features based on a predefined distance function</strong></p>
<ul>
<li>Feature Matching: Given a feature and its descriptor in image 1, find the best match in image 2</li>
<li>Define a distance function $d(f_i,f_j)$ that compares the two descriptors</li>
<li>For every feature $f_i$ in Image 1: <ul>
<li>Compute $d(f_i,f_j)$ with all features $f_j$ in image 2</li>
<li>Find the closest match $f_c$, the match that has the minimum distance. This feature is known as the nearest neighbor.</li>
<li>Keep this match only if $d(f_i,f_j)$ is below threshold $\delta$</li>
</ul>
</li>
<li>Distance Function<ul>
<li>Sum of Squared Differences (SSD)： $d(f_i, f_j) =\sum^D_{k=1} (f_{i,k} - f_{j,k})^2$</li>
<li>Sum of absolute differences (SAD): $d(f_i, f_j) = \sum^D_{k=1} |f_{i,k} - f_{j,k}|$</li>
<li>Hamming Distance: $d(f_i, f_j) = \sum^D_{k=1} XOR(f_{i,k} - f_{j,k})$</li>
</ul>
</li>
<li>Brute force feature matching might not be fast enough for extremely large amounts of features<ul>
<li>Use a multidimensional search tree, usually a k-d tree to speed the search by constraining it spatially</li>
<li>Both of these matchers are implemented in OpenCV as: <code>cv2.BFMatcher()</code>: Brute force matcher; <code>cv2.FlannBasedMatcher()</code>: K-D tree based approximate nearest neighbor matcher</li>
</ul>
</li>
</ul>
<h3 id="Feature-Matching-Handling-Ambiguity-in-Matching"><a href="#Feature-Matching-Handling-Ambiguity-in-Matching" class="headerlink" title="Feature Matching: Handling Ambiguity in Matching"></a>Feature Matching: Handling Ambiguity in Matching</h3><ul>
<li>Ambiguous matches: more than one feature points can be found to have the minimum distance when do feature matching. </li>
<li>The solution is to use distance ratio:<ul>
<li>Compute $d(f_i,f_j)$ for each feature, $f_i$, in image 1, with all features, $f_j$, in image 2</li>
<li>Find the closest match $f_c$</li>
<li>Find the second closest match $f_s$</li>
<li>Find how better the closest match is than the second closest match. This can be done through distance ratio: $0 \leq \frac{d(f_i,f_c)}{d(f_i,f_s)} \leq 1$</li>
<li>If the distance ratio is close to one, it means that according to our descriptor and distance function, fi matches both fs and fc. In this case, we don’t want to use this match in our processing later on, as it clearly is not known to our matcher which location in image two corresponds to the feature in image one.</li>
</ul>
</li>
<li>So the Updated Brute Force Feature Matching: <ul>
<li>Define a distance function $d(f_i,f_j)$ that compares the two descriptors</li>
<li>Define distance ratio threshold $\rho$</li>
<li>For every feature $f_i$ in Image 1:<ul>
<li>Compute $d(f_i,f_j)$ with all features $f_j$ in image 22. </li>
<li>Find the closest match $f_c$ and the second closest match $f_s$ </li>
<li>Compute the distance ratio </li>
<li>Keep matches with distance ratio $&lt; \rho$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Outlier-Rejection"><a href="#Outlier-Rejection" class="headerlink" title="Outlier Rejection"></a>Outlier Rejection</h3><p><strong>Three-step feature extraction framework for the real-world problem of vehicle localization</strong></p>
<ul>
<li>Localization problem is defined as follows: <ul>
<li>given any two images of the same scene from different perspectives, find the translation $T=[t_u, t_v]$, between the coordinate system of the first image , and the coordinate system of the second image.</li>
<li>also need to solve for the scale and skew due to different viewpoints. </li>
</ul>
</li>
<li>Matched feature pairs in images 1 and 2: <ul>
<li>$f^{(1)}_i, f^{(2)}_i i \in [0…N]$</li>
<li>$f^{(1)}_i = (u^{(1)}_i, v^{(1)}_i)$</li>
<li>Model: $u^{(1)}_i + t_u = u^{(2)}_i$, $v^{(1)}_i + t_v = v^{(2)}_i$</li>
<li>solve using least squares: $t_u = \frac 1N \sqrt{\sum_i (u^{(1)}_i - u^{(2)}_i)^2}$  $t_v = \frac 1N \sqrt{\sum_i (v^{(1)}_i - v^{(2)}_i)^2}$</li>
</ul>
</li>
</ul>
<p><strong>Outliers and ANSAC algorithm</strong></p>
<ul>
<li>Outliers can be handled using a model-based outlier rejection method called Random Sample Consensus (RANSAC)<br>RANSAC algorithm: </li>
<li>Initialization:<ul>
<li>Given a model, find the smallest number M of data points or samples needed to compute the parameters of this model. In the above case is $t_u, t_v$</li>
</ul>
</li>
<li>Main loop:<ul>
<li>Randomly select M samples from the data.</li>
<li>Compute the model parameters using only the M samples selected from the data set. </li>
<li>Use the computed parameters and count how many of the remaining data points agree with this computed solution. The accepted points are retained and referred to as inliers.</li>
<li>if the number of inliers C is satisfactory, or if the algorithm has iterated a pre-set maximum number of iterations, terminate and return the computed solution and the inlier set. </li>
<li>Else, go back to step two and repeat.</li>
</ul>
</li>
<li>Finally, recompute and return the model parameters from the best inlier set: The one with the largest number of features. </li>
</ul>
<h3 id="Visual-Odometry"><a href="#Visual-Odometry" class="headerlink" title="Visual Odometry"></a>Visual Odometry</h3><p>Visual Odometry(VO): is the process of incrementally estimating the pose of the vehicle by examining the changes that motion induces on the images of its onboard cameras</p>
<ul>
<li>VO Pros:<ul>
<li>Not affected by wheel slip in uneven terrain, rainy/snowy weather, or other adverse conditions.</li>
<li>More accurate trajectory estimates compared to wheel odometry.</li>
</ul>
</li>
<li>VO Cons:<ul>
<li>Usually need an external sensor to estimate absolute scale</li>
<li>Camera is a passive sensor,might not be very robust against weather conditions and illumination changes</li>
<li>Any form of odometry (incremental state estimation) drifts over time</li>
</ul>
</li>
<li>Problem Formulation:<ul>
<li>Estimate the camera motion $T_k$ between consecutive images $l_{k-1}$ and $l_k$</li>
<li>Concatenating these single movements allows the recovery of the full trajectory of the camera, given frames $C_1, …, C_m$</li>
</ul>
</li>
<li>General process of visual odometry:<ul>
<li>Given: two consecutive image frames $I_{k-1}$ and $I_k$</li>
<li>First, perform feature detection and description. We end up with a set of features $f_{k-1}$ in image k-1 and $f_k$ in image of k. </li>
<li>We then proceed to match the features between the two frames to find the ones occurring in both of our target frames.</li>
<li>After that, we use the matched features to estimate the motion between the two camera frames represented by the transformation $T_k$.</li>
</ul>
</li>
<li>Motion estimation:<ul>
<li>depends on what type of feature representation we have:<ul>
<li>2D-2D: both $f_{k-1}$ and $f_k$ are defined in Image coordinates</li>
<li>3D-3D: both  $f_{k-1}$ and $f_k$ are specified in 3D</li>
<li>3D-2D: $f_{k-1}$ is specified in 3D and $f_k$ are their corresponding projection on 2D</li>
</ul>
</li>
</ul>
</li>
<li>Perspective N Point (PNP)<ul>
<li>Given feature locations in 3D, their corresponding projection in 2D, and the camera intrinsic calibration matrix k,</li>
<li>Solve for initial guess of [R|t] using Direct Linear Transform(DLT): Forms a linear model and solves for [R|t], with methods such as singular value decomposition(SVD)</li>
<li>Improve solution using Levenberg-Marquardt algorithm(LM)</li>
<li>Need at least 3 points to solve(P3P), 4 if we don’t wantambiguous solutions.</li>
<li>Finally, RANSAC can be incorporated into PnP by assuming that the transformation generated by PnP on four points is our model. </li>
<li>We then choose a subset of all feature matches to evaluate this model and check the percentage of inliers that result to confirm the validity of the point matches selected. </li>
</ul>
</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Learning</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Self-driving</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2019/08/06/coursera_visual_percep02/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-coursera_visual_percep01" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/01/coursera_visual_percep01/">Coursera Visual Perception for Self-Driving Cars 01</a>
    </h1>
  

        
        <a href="/2019/08/01/coursera_visual_percep01/" class="archive-article-date">
  	<time datetime="2019-08-01T13:30:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-08-01</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!--  
            <div id="toc" class="toc-article">
              <strong class="toc-title">文章目录</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basics-of-3D-Computer-Vision"><span class="toc-number">1.</span> <span class="toc-text">Basics of 3D Computer Vision</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Camera-Sensor"><span class="toc-number">1.1.</span> <span class="toc-text">The Camera Sensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Camera-Projective-Geometry"><span class="toc-number">1.2.</span> <span class="toc-text">Camera Projective Geometry</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Camera-Calibration"><span class="toc-number">1.3.</span> <span class="toc-text">Camera Calibration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Visual-Depth-Perception-Stereopsis"><span class="toc-number">1.4.</span> <span class="toc-text">Visual Depth Perception - Stereopsis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Visual-Depth-Perception-Computing-the-Disparity"><span class="toc-number">1.5.</span> <span class="toc-text">Visual Depth Perception - Computing the Disparity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Image-filtering"><span class="toc-number">1.6.</span> <span class="toc-text">Image filtering</span></a></li></ol></li></ol>
            </div>
           -->
        <!--  
    <div id="toc-container"> 
        <div id="toc"> 
            <p> 
                <strong>文章目录</strong>
            </p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basics-of-3D-Computer-Vision"><span class="toc-text">Basics of 3D Computer Vision</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Camera-Sensor"><span class="toc-text">The Camera Sensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Camera-Projective-Geometry"><span class="toc-text">Camera Projective Geometry</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Camera-Calibration"><span class="toc-text">Camera Calibration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Visual-Depth-Perception-Stereopsis"><span class="toc-text">Visual Depth Perception - Stereopsis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Visual-Depth-Perception-Computing-the-Disparity"><span class="toc-text">Visual Depth Perception - Computing the Disparity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Image-filtering"><span class="toc-text">Image filtering</span></a></li></ol></li></ol>
        </div>
    </div>
 -->
        <p>From Coursera, State Estimation and Localization for Self-Driving Cars by University of Toronto<br><a href="https://www.coursera.org/learn/visual-perception-self-driving-cars" target="_blank" rel="noopener">https://www.coursera.org/learn/visual-perception-self-driving-cars</a></p>
<h2 id="Basics-of-3D-Computer-Vision"><a href="#Basics-of-3D-Computer-Vision" class="headerlink" title="Basics of 3D Computer Vision"></a>Basics of 3D Computer Vision</h2><h3 id="The-Camera-Sensor"><a href="#The-Camera-Sensor" class="headerlink" title="The Camera Sensor"></a>The Camera Sensor</h3><p>Pinhole Camera Model:</p>
<ul>
<li>focal length: the distance between the pinhole and the image plane. It defines the size of the object projected onto the image </li>
<li>camera center: The coordinates of the center of the pinhole. These coordinates defined the location on the imaging sensor that the object projection will inhabit. </li>
</ul>
<h3 id="Camera-Projective-Geometry"><a href="#Camera-Projective-Geometry" class="headerlink" title="Camera Projective Geometry"></a>Camera Projective Geometry</h3><p>Let’s define the problem we need to solve: a point $O_{world}$ defined at a particular location in the world coordinate frame. We want to project this point from the world frame to the camera image plane. </p>
<ul>
<li>Light travels from the $O_{world}$ on the object through the camera aperture to the sensor surface. </li>
<li>The projection onto the sensor surface through the aperture results in flipped images of the objects in the world.</li>
<li>need to develop a model for how to project a point from the world frame coordinates x, y and z to, image coordinates u and v:<ul>
<li>First, select a <em>world frame</em> in which to define the coordinates of all objects and the camera.</li>
<li>define the <em>camera coordinate frame</em> as the coordinate frame attached to the center of our lens aperture known as the optical sensor.</li>
<li>We refer to the parameters of the camera pose as the extrinsic parameters, as they are external to the camera and specific to the location of the camera in the world coordinate frame. </li>
<li>define <em>image coordinate frame</em> as the coordinate frame attached to our virtual image plane emanating from the optical center. The image pixel coordinate system is attached to the top <em>left corner</em> of the virtual image plane.</li>
<li>So we need to adjust the pixel locations to the image coordinate frame.</li>
<li>we define the focal length is the distance between the camera and the image coordinate frames along the z-axis of the camera coordinate frame.</li>
</ul>
</li>
<li>Finally, the projection problem reduces to two steps. <ul>
<li>We first need to project from the world to the camera coordinates, then we project from the camera coordinates to the image coordinates.</li>
<li>We can then transform image coordinates to pixel coordinates through scaling and offset.</li>
</ul>
</li>
<li>Computing the projection:<ul>
<li>World -&gt; Camera: $$o_{camera} = [R|t]O_{world}$$</li>
<li>Camera -&gt; Image: $$o_{image} = [f \; 0 \; u_0 ; 0 \; f \; v_0 ; 0 \; 0 \; 1]o_{camera} = Ko_{camera}$$<ul>
<li>K is a 3x3 matrix, which depends on camera intrinsic parameters: camera geometry and the camera lens characteristics</li>
</ul>
</li>
<li>World -&gt; Image: $$P = K[R|t]$$</li>
<li>therefore, $o_{image} = PO = K[R|t]O_{world}$</li>
</ul>
</li>
<li>Image coordinates to Pixel coordinates: $[x y z]^T -&gt; [u v 1]^T = \frac 1z[x y z]^T$</li>
</ul>
<p>The digital image:</p>
<ul>
<li>an image is represented digitally as an M by N by three array of pixels, with each pixel representing the projection of a 3D point onto the 2D image plane. </li>
</ul>
<h3 id="Camera-Calibration"><a href="#Camera-Calibration" class="headerlink" title="Camera Calibration"></a>Camera Calibration</h3><p>The camera calibration problem is defined as finding these unknown intrinsic and extrinsic camera parameters, given n known 3D point coordinates and their corresponding projection to the image plane.</p>
<ul>
<li>Our approach will comprise of getting the P matrix first and then decomposing it into the intrinsic parameters K and the extrinsic rotation parameters R and translation parameters t. </li>
<li>Use scenes with known geometry to:<ul>
<li>Correspond 2D image coordinates to 3D world coordinates</li>
<li>Find the Least Squares Solution (or non-linear solution)of the parameters of P</li>
</ul>
</li>
<li>The most commonly used example would be a 3D checkerboard, with squares of known size providing a map of fixed point locations to observe. </li>
<li>If we have N 3D points and their corresponding N 2D projections, set up homogeneous linear system<ul>
<li>Solved with Singular Value Decomposition(SVD)</li>
</ul>
</li>
</ul>
<h3 id="Visual-Depth-Perception-Stereopsis"><a href="#Visual-Depth-Perception-Stereopsis" class="headerlink" title="Visual Depth Perception - Stereopsis"></a>Visual Depth Perception - Stereopsis</h3><p><strong>Stereo Camera Model</strong></p>
<ul>
<li>A stereo sensor is usually created by two cameras with parallel optical axes.</li>
<li>Given a known rotation and translation between the two cameras and a known projection of a point $O$ in 3D to the two camera frames resulting in pixel locations $O_L$ and $O_R$ respectively, we can formulate the necessary equations to compute the 3D coordinates of the point $O$.</li>
<li>Assumptions:<ul>
<li>First, we assume that the two cameras used to construct the stereo sensors are identical. </li>
<li>Second, we will assume that while manufacturing the stereo sensor, we tried as hard as possible to keep the two cameras optical axes aligned.</li>
<li>project the previous figure to bird’s eye view for easier visualization. </li>
</ul>
</li>
<li>Some parameters:<ul>
<li>focal length: the distance between the camera center and the image plane.</li>
<li>the baseline is defined as the distance along the shared x-axis between the left and right camera centers. </li>
<li>By defining a baseline to represent the transformation between the two camera coordinate frames, we are assuming that the rotation matrix is identity and there is only a non-zero x component in the translation vector. The $[R|t]$ transformation therefore boils down to a single baseline parameter B.</li>
</ul>
</li>
<li>Define the quantities to compute:<ul>
<li>to compute the x and z coordinates of the point $O$ with respect to the left camera frame.</li>
<li>The y coordinate can be estimated easily after the x and z coordinates are computed. </li>
<li>by constructing the similarity, we get $\frac Zf = \frac X{x_L}$ and $\frac Zf = \frac {X-b}{x_R}$</li>
<li>finally we can get: $X=\frac{zx_L}{f}$, $Y=\frac{zy_L}{f}$</li>
</ul>
</li>
</ul>
<p><strong>Derive the location of a point in 3D</strong></p>
<ul>
<li>Two main problems: <ul>
<li>We need to know $f,b,u_o,v_o$: Use stereo camera calibration </li>
<li>We need to find corresponding $x_R$ for each $x_L$: Use disparity computation algorithms</li>
</ul>
</li>
</ul>
<h3 id="Visual-Depth-Perception-Computing-the-Disparity"><a href="#Visual-Depth-Perception-Computing-the-Disparity" class="headerlink" title="Visual Depth Perception - Computing the Disparity"></a>Visual Depth Perception - Computing the Disparity</h3><p>Disparity: The difference in image location of the same 3D point under perspective to two different cameras</p>
<ul>
<li>Correspond pixels in the left image to those in the right image to find matches</li>
</ul>
<p><strong>Estimate the disparity through stereo matching</strong></p>
<ul>
<li>if moving our 3D point along the line connecting it with the left cameras center. <ul>
<li>Its projection on the left camera image plane does not change. But for the projection on the right camera plane, the projection moves along the horizontal line. </li>
<li>This is called an epipolar line and follows directly from the fixed lateral offset and image plane alignment of the two cameras in a stereo pair. We can constrain our correspondence search to be along the epipolar line, reducing the search from 2D to 1D.</li>
<li>One thing to note is that horizontal epipolar lines only occur if the optical axes of the two cameras are parallel. </li>
<li>In the case of non parallel optical axis, the epipolar lines are skewed.</li>
<li>We can use <em>stereo rectification</em> to warpimages originating from two cameras with non-parallel optical axes to force epipolar lines to be horizontal.</li>
</ul>
</li>
<li>A Basic Stereo Algorithm <ul>
<li>Given: Rectified Images and Stereo Calibration.</li>
<li>For each epipolar line take a pixel on this line in the left image, compare these left image pixels to every pixel in the right image on the same epipolar line.</li>
<li>select the right image pixel that matches the left pixel the most closely which can be done by minimizing the cost. </li>
<li>a very simple cost can be the squared difference in pixel intensities.</li>
<li>Finally, we can compute the disparity by subtracting the right image location from the left one.</li>
</ul>
</li>
</ul>
<h3 id="Image-filtering"><a href="#Image-filtering" class="headerlink" title="Image filtering"></a>Image filtering</h3><p><strong>Cross correlation</strong></p>
<ul>
<li>The idea to reduce salt-pepper noise is to compute the mean of the whole neighborhood, and replace the outlier pixel with this mean value: $$G[u,v] = \frac 1{(2k+1)^2} \sum^k_{i=-k} \sum^k_{j=-k} I[u-i, v-j]$$ <ul>
<li>where (2k+1) is the filter size, (u,v) is the center pixel coordinates</li>
</ul>
</li>
<li>The mean equation can be generalized by adding a weight to every pixel in the neighborhood, resulting in cross-correlation. The weight matrix H is called a kernel. <ul>
<li>Kernal could be: mean filter, gaussian filter</li>
</ul>
</li>
<li>Application：<ul>
<li>Template matching: The pixel with the highest response from Cross-correlation is the location of the template in an image</li>
<li>Image gradient computation: Define a finite difference kernel,and apply it to the image to get the image gradient</li>
</ul>
</li>
</ul>
<p><strong>Convolution</strong></p>
<ul>
<li>A convolution is a cross-correlation where the filter is flipped both horizontally and vertically before being applied to the image</li>
<li>Unlike Cross-Correlation, Convolution is associative. If H and F are filter kernels then: $H<em>(F</em>I) = (H<em>F)</em>I$</li>
<li>Precompute filter convolutions(H*F)then apply it once to the image to reduce runtime.</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Learning</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Self-driving</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2019/08/01/coursera_visual_percep01/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-coursera_state_est05" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/01/coursera_state_est05/">Coursera State Estimation and Localization for Self-Driving Cars 05</a>
    </h1>
  

        
        <a href="/2019/08/01/coursera_state_est05/" class="archive-article-date">
  	<time datetime="2019-08-01T08:57:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-08-01</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!--  
            <div id="toc" class="toc-article">
              <strong class="toc-title">文章目录</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#An-Autonomous-Vehicle-State-Estimator"><span class="toc-number">1.</span> <span class="toc-text">An Autonomous Vehicle State Estimator</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#State-Estimation-in-Practice"><span class="toc-number">1.1.</span> <span class="toc-text">State Estimation in Practice</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multisensor-Fusion-for-State-Estimation"><span class="toc-number">1.2.</span> <span class="toc-text">Multisensor Fusion for State Estimation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sensor-Calibration-A-Necessary-Evil"><span class="toc-number">1.3.</span> <span class="toc-text">Sensor Calibration - A Necessary Evil</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss-of-One-or-More-Sensors"><span class="toc-number">1.4.</span> <span class="toc-text">Loss of One or More Sensors</span></a></li></ol></li></ol>
            </div>
           -->
        <!--  
    <div id="toc-container"> 
        <div id="toc"> 
            <p> 
                <strong>文章目录</strong>
            </p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#An-Autonomous-Vehicle-State-Estimator"><span class="toc-text">An Autonomous Vehicle State Estimator</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#State-Estimation-in-Practice"><span class="toc-text">State Estimation in Practice</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multisensor-Fusion-for-State-Estimation"><span class="toc-text">Multisensor Fusion for State Estimation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sensor-Calibration-A-Necessary-Evil"><span class="toc-text">Sensor Calibration - A Necessary Evil</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss-of-One-or-More-Sensors"><span class="toc-text">Loss of One or More Sensors</span></a></li></ol></li></ol>
        </div>
    </div>
 -->
        <p>From Coursera, State Estimation and Localization for Self-Driving Cars by University of Toronto<br><a href="https://www.coursera.org/learn/state-estimation-localization-self-driving-cars" target="_blank" rel="noopener">https://www.coursera.org/learn/state-estimation-localization-self-driving-cars</a></p>
<h2 id="An-Autonomous-Vehicle-State-Estimator"><a href="#An-Autonomous-Vehicle-State-Estimator" class="headerlink" title="An Autonomous Vehicle State Estimator"></a>An Autonomous Vehicle State Estimator</h2><h3 id="State-Estimation-in-Practice"><a href="#State-Estimation-in-Practice" class="headerlink" title="State Estimation in Practice"></a>State Estimation in Practice</h3><p>Accuracy Requirements</p>
<ul>
<li>How accurate does the estimator need to be for safe self-driving?</li>
<li>Typically less than a meter for highway lane keeping</li>
<li>Less for driving in dense trafic</li>
<li>GPS accuracy is 1-5 meters in optimal conditions</li>
<li>Need additional sensors!</li>
</ul>
<p>Speed Requirements</p>
<ul>
<li>How fast do we need to update the vehicle state to ensure safe driving?</li>
<li>How much computation power does the vehicle have on-board?</li>
<li>How much power can our computing resources consume?</li>
</ul>
<p>Localization Failures</p>
<ul>
<li>How can localization fail?</li>
<li>Sensors fail or provide bad data (e.g., GPS in a tunnel)</li>
<li>Estimation error (e.g, linearization error in the EKF)</li>
<li>Large state uncertainty (e.g, relying on IMU for too long)</li>
</ul>
<h3 id="Multisensor-Fusion-for-State-Estimation"><a href="#Multisensor-Fusion-for-State-Estimation" class="headerlink" title="Multisensor Fusion for State Estimation"></a>Multisensor Fusion for State Estimation</h3><p>Develop an error state extended Kalman Filter for estimating position, velocity and orientation using an IMU, GNSS sensor, and LIDAR.</p>
<p><strong>Why use GNSS with IMU &amp; LIDAR?</strong></p>
<ul>
<li>Eror dynamics are completely different and uncorrelated</li>
<li>IMU provides ‘smoothing’ of GNSS, fill-in during outages due to jamming or maneuvering</li>
<li>Wheel odometry is also possible (if only 20 position orientation is desired)· </li>
<li>GNSS provides absolute positioning information to mitigate IMU drift</li>
<li>LIDAR provides accurate local positioning within known maps</li>
</ul>
<p><strong>Types of EKF coupling</strong></p>
<ul>
<li>Tightly coupling:<ul>
<li>use the raw pseudo range and point cloud measurements from our GNSS and LIDAR as observations</li>
<li>GNSS/LIDAR Measurement: Pseudo-ranges to satellites LIDAR point clouds</li>
<li>Accuracy: potentially Higher</li>
<li>Complexity: Higher</li>
</ul>
</li>
<li>Loosely<ul>
<li>assume data has already been preprocessed to produce a noisy position estimate</li>
<li>GNSS/LIDAR Measurement: Position</li>
<li>Accuracy: potentially lower</li>
<li>Complexity: Lower</li>
</ul>
</li>
</ul>
<p><strong>EKF - IMU+GNSS+LIDAR</strong></p>
<ul>
<li>use the IMU measurements as noisy inputs to the motion model. This will give us our predicted state, which will update every time we have an IMU measurement. This can happen hundreds of times a second.</li>
<li>Then we incorporate GNSS and LIDAR measurements whenever they become available(at a much slower rate,  say once a second or slower), and use them to correct our predicted state</li>
</ul>
<p><img src="/images/state_est07.jpg" width="80%" height="80%"></p>
<ul>
<li>What is state?<ul>
<li>we’ll use a 10-dimensional state vector that includes a 3D position, a 3D velocity, and a 4D unit quaternion that will represent the orientation of our vehicle with respect to a navigation frame. $$x_k=[p_k \; v_k \; q_k]^T \in R^{10}$$</li>
<li>assume that IMU output specific forces and rotational rates in the sensor frame, and combine them into a single input vector u. - note that we’re not going to track accelerometer or gyroscope biases. These are often put into the state vector, estimated, and then subtracted off of the IMU measurements. For clarity, we’ll emit them here and assume our IMU measurements are unbiased.</li>
<li>therefore, the motion model input will consist of specific force and rotational rates from IMU: $$u_k = [f_k \; \omega_k]^T \in R^6$$</li>
</ul>
</li>
<li>Loop<ul>
<li>Update state with IMU inputs</li>
<li>Propagate uncertainty</li>
<li>If GNSS or LIDAR position available:<ul>
<li><ol>
<li>Compute Kalman gain</li>
</ol>
</li>
<li><ol start="2">
<li>Compute error state</li>
</ol>
</li>
<li><ol start="3">
<li>Correct predicted state</li>
</ol>
</li>
<li><ol start="4">
<li>Computed orrected covariance</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Sensor-Calibration-A-Necessary-Evil"><a href="#Sensor-Calibration-A-Necessary-Evil" class="headerlink" title="Sensor Calibration - A Necessary Evil"></a>Sensor Calibration - A Necessary Evil</h3><p><strong>Intrinsic Calibration</strong></p>
<ul>
<li>deals with sensors specific parameters</li>
<li>ways to get the parameters:<ul>
<li>Manufacturer specifications</li>
<li>Measure by hand</li>
<li>Estimate as part of the state</li>
</ul>
</li>
</ul>
<p><strong>Extrinsic Calibration</strong> </p>
<ul>
<li>deals with how the sensors are positioned and oriented on the vehicle</li>
</ul>
<p><strong>Temporal Calibration</strong></p>
<ul>
<li>deals with the time offset between different sensor measurements</li>
<li>ways to deal with<ul>
<li>Assume zero</li>
<li>Hardware synchronization</li>
<li>Estimate as part of the state</li>
</ul>
</li>
</ul>
<h3 id="Loss-of-One-or-More-Sensors"><a href="#Loss-of-One-or-More-Sensors" class="headerlink" title="Loss of One or More Sensors"></a>Loss of One or More Sensors</h3>
      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Learning</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Self-driving</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2019/08/01/coursera_state_est05/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-coursera_state_est04" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/31/coursera_state_est04/">Coursera State Estimation and Localization for Self-Driving Cars 04</a>
    </h1>
  

        
        <a href="/2019/07/31/coursera_state_est04/" class="archive-article-date">
  	<time datetime="2019-07-31T08:57:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-07-31</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!--  
            <div id="toc" class="toc-article">
              <strong class="toc-title">文章目录</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#LIDAR-Sensing"><span class="toc-number">1.</span> <span class="toc-text">LIDAR Sensing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Light-Detection-and-Ranging-Sensors"><span class="toc-number">1.1.</span> <span class="toc-text">Light Detection and Ranging Sensors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LIDAR-Sensor-Models-and-Point-Clouds"><span class="toc-number">1.2.</span> <span class="toc-text">LIDAR Sensor Models and Point Clouds</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pose-Estimation-from-LIDAR-Data"><span class="toc-number">1.3.</span> <span class="toc-text">Pose Estimation from LIDAR Data</span></a></li></ol></li></ol>
            </div>
           -->
        <!--  
    <div id="toc-container"> 
        <div id="toc"> 
            <p> 
                <strong>文章目录</strong>
            </p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#LIDAR-Sensing"><span class="toc-text">LIDAR Sensing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Light-Detection-and-Ranging-Sensors"><span class="toc-text">Light Detection and Ranging Sensors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LIDAR-Sensor-Models-and-Point-Clouds"><span class="toc-text">LIDAR Sensor Models and Point Clouds</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pose-Estimation-from-LIDAR-Data"><span class="toc-text">Pose Estimation from LIDAR Data</span></a></li></ol></li></ol>
        </div>
    </div>
 -->
        <p>From Coursera, State Estimation and Localization for Self-Driving Cars by University of Toronto<br><a href="https://www.coursera.org/learn/state-estimation-localization-self-driving-cars" target="_blank" rel="noopener">https://www.coursera.org/learn/state-estimation-localization-self-driving-cars</a></p>
<h2 id="LIDAR-Sensing"><a href="#LIDAR-Sensing" class="headerlink" title="LIDAR Sensing"></a>LIDAR Sensing</h2><h3 id="Light-Detection-and-Ranging-Sensors"><a href="#Light-Detection-and-Ranging-Sensors" class="headerlink" title="Light Detection and Ranging Sensors"></a>Light Detection and Ranging Sensors</h3><p><strong>The operating principles of LIDAR sensors</strong></p>
<ul>
<li>For a basic LIDAR in one dimension, it has three components: a laser, a photodetector, and a very precise stopwatch. </li>
<li>The laser first emits a short pulse of light usually in the near infrared frequency band along some known ray direction. At the same time, the stopwatch begins counting. The laser pulse travels outwards from the sensor at the speed of light and hits a distant target. </li>
<li>As long as the surface of the target isn’t too polished or shiny, the laser pulse will scatter off the surface in all directions, and some of that reflected light will travel back along the original ray direction. The photodetector catches that return pulse and the stopwatch tells you how much time has passed between when the pulse first went out and when it came back. That time is called the round-trip time. </li>
<li>then the distance from the LIDAR to the target is simply half of the round-trip distance calculated by the time and the speed of the light</li>
<li>This technique is called time-of-flight ranging</li>
<li>the photodetector also tells the intensity of the return pulse relative to the intensity of the pulse that was emitted. This intensity information is less commonly used for self-driving, but it provides some extra information about the geometry of the environment and the material the beam is reflecting off of.</li>
</ul>
<p><strong>The basic LIDAR sensor models in 2D and 3D</strong></p>
<ul>
<li>But how do to use the above knowledge to measure a whole bunch of distances in 2D or in 3D? <ul>
<li>The trick is to build a rotating mirror into the LIDAR that directs the emitted pulses along different directions. </li>
<li>As the mirror rotates, you can measure distances at points in a 2D slice around the sensor. </li>
<li>If you then add an up and down nodding motion to the mirror along with the rotation, you can use the same principle to create a scan in 3D. </li>
</ul>
</li>
<li>Measurement Models for 3D LIDAR Sensors<ul>
<li>LIDARs measure the position of points in 3D using spherical coordinates, range or radial distance from the center origin to the 3D point, elevation angle measured up from the sensors XY plane, and azimuth angle, measured counterclockwise from the sensors x-axis.</li>
<li>The azimuth and elevation angles are measured using encoders that tell you the orientation of the mirror, and the range is measured using the time of flight as we’ve seen before.</li>
<li>suppose we want to determine the cartesian XYZ coordinates of our scanned point in the sensor frame, which is something we often want to do when we’re combining multiple LIDAR scans into a map. To convert from spherical to Cartesian coordinates, we use the inverse sensor model: $$[x \; y \; z]^T = h^{-1}(r,\alpha, \varepsilon) = [r\cos\alpha \cos \varepsilon \quad r \sin \alpha \cos \varepsilon \quad r \sin \varepsilon]^T$$ where $\alpha$ is the azimuth angle, $\varepsilon$ is the elevation angle</li>
<li>Therefore, the forward sensor model is: (which from Cartesian coordinates to spherical coordinates) $$[r \alpha \varepsilon]^T = h(x,y,z) = [\sqrt{x^2+y^2+z^2} \tan^{-1}(\frac yx) \sin^{-1}(\frac{z}{\sqrt{x^2+y^2+z^2}})]^T$$</li>
</ul>
</li>
</ul>
<p><img src="/images/state_est02.jpg" width="80%" height="80%"></p>
<p><strong>The major sources of measurement error for LIDAR sensors</strong></p>
<ul>
<li>Uncertainty in determining the exact time of arrival of the refected signal</li>
<li>Uncertainty in measuring the exact orientation of the mirror</li>
<li>Interaction with the target(surface absorption, specular reflection, etc.): e.g.: if the surface is completely black, it might absorb most of the laser pulse. Or if it’s very shiny like a mirror, the laser pulse might be scattered completely away from the original pulse direction.</li>
<li>Variation of propagation speed (e.g., through materials)</li>
<li>These factors are commonly accounted for by assuming additive zero-mean Gaussian noise on the spherical coordinates with an empirically determined or manually tuned covariance. </li>
<li>Motion Distortion<ul>
<li>Typical scan rate for a 3D LIDAR is 5-20 Hz</li>
<li>For a moving vehicle, each point in a scan is taken from a slightly different place</li>
<li>Need to account for this if the vehicle is moving quickly, otherwise motion distortion becomes a problem</li>
</ul>
</li>
</ul>
<h3 id="LIDAR-Sensor-Models-and-Point-Clouds"><a href="#LIDAR-Sensor-Models-and-Point-Clouds" class="headerlink" title="LIDAR Sensor Models and Point Clouds"></a>LIDAR Sensor Models and Point Clouds</h3><p><strong>The basic point cloud data structure</strong></p>
<ul>
<li>assign an index to each of the points, say point 1 through point n, and store the x, y, z coordinates of each point as a 3 by 1 column vector</li>
</ul>
<p><strong>Common spatial operations on point clouds</strong></p>
<ul>
<li>Translation<br><img src="/images/state_est03.jpg" width="70%" height="70%"></li>
<li>Rotation<br><img src="/images/state_est04.jpg" width="70%" height="70%"></li>
<li>Scaling<br><img src="/images/state_est05.jpg" width="70%" height="70%"></li>
<li>Put them together<br><img src="/images/state_est06.jpg" width="70%" height="70%"></li>
</ul>
<p><strong>Least squares to fit a plane to a point cloud</strong></p>
<ul>
<li>Plane fitting</li>
<li>One of the most common and important applications of plane-fitting for self-driving cars is figuring out where the road surface is and predicting where it’s going to be as the car continues driving.<ul>
<li>we have a bunch of measurements of x, y and z from our LIDAR point cloud, and we want to find values for the plane by the parameters a, b, and c defined: $z=a+bx+cy$</li>
<li>to find the best fit, we use least-squares estimation</li>
<li>define a measurement error $e$ for each point in the point cloud: $e_j = \hat z_j - z_j = (\hat a + \hat bx_j + \hat c y_j) - z_j \quad j=1,…n$ </li>
<li>We can stack all of the measurement errors into matrix form, ad minimize the squared-error criterion to get the least-squares solutions for the parameters</li>
</ul>
</li>
<li>Open-source Point Cloud Library(PCL) has many useful functions for doing basic and advanced operations on point clouds in C++</li>
</ul>
<h3 id="Pose-Estimation-from-LIDAR-Data"><a href="#Pose-Estimation-from-LIDAR-Data" class="headerlink" title="Pose Estimation from LIDAR Data"></a>Pose Estimation from LIDAR Data</h3><p><strong>Point set registration problem</strong></p>
<ul>
<li>one of the most important problems in computer vision and pattern recognition. used to estimate the motion of a self-driving car from point clouds </li>
<li>the point set registration problems says, given 2 point clouds in two different coordinate frames, and with the knowledge that they correspond to or contain the same object in the world, how shall we align them to determine how the sensor must have moved between the two scans? </li>
<li>More specifically, we want to figure out the optimal translation and the optimal rotation between the two sensor reference frames that minimizes the distance between the 2 point clouds.</li>
<li>ICP is the most popular algorithm to solve this problem.</li>
</ul>
<p><strong>Iterative Closest Point(ICP) algorithm</strong></p>
<ul>
<li>Intuition: When the optimal motion is found, corresponding points will be closer to each other than to other points</li>
<li>Heuristic:For each point,the best candidate for a corresponding point is the point that is closest to it right now</li>
<li>Steps of ICP:<ul>
<li>Get an initial guess for the transformation ${\check C_{S’S}, \check r^{S’S}_{S}}$: <ul>
<li>the initial guess can come from a number of sources. </li>
<li>One of the most common sources is a motion model, which could be supplied by an IMU or by wheel odometry or something really simple like a constant velocity or even a zero velocity model</li>
<li>How complex the motion model needs to be to give us a good initial guess really depends on how smoothly the car is driving.</li>
<li>If the car is moving slowly relative to the scanning rate of the LIDAR sensor, one may even use the last known pose as the initial guess.</li>
</ul>
</li>
<li>Associate each point in $P_{S’}$ with the nearest point in $P_S: transform the coordinates of the points in one cloud into the reference frame of the other</li>
<li>Solve for the optimal transformation ${\hat C_{S’S}, \hat r^{S’S}_S}$</li>
<li>Repeat until convergence</li>
</ul>
</li>
<li>Solving for the Optimal Transformation<ul>
<li>use least-squares:</li>
<li>Special attention need to be paid to the rotation matrix: as two rotation matrices addition will not necessarily results in a valid rotation matrix. 3D rotations belong to something called the special orthogonal group or SO3</li>
</ul>
</li>
<li>Steps:<ul>
<li>Compute the centroids of each point cloud: <ul>
<li>$\mu_S = \frac 1n \sum^n_{j=1}P^{j}_S$ </li>
<li>$\mu_{S’} = \frac 1n\sum^n_{j=1}P^{(j)}_{S’}$</li>
</ul>
</li>
<li>Compute a matrix capturing the spread of the two point clouds: $$W_{S’S} = \frac 1n \sum^n_{j=1}(P^{(j)}{s} - \mu_{S})(P^{(j)}{s’}-\mu_{S’})^T$$ this W matrix can be regarded as something like an inertia matrix you might encounter in mechanics. </li>
<li>finding the optimal rotation matrix using SVD of W matrix: $$USV^T = W_{S’S}$$ where U V are rotation and S is the scaling matrix. As we’re dealing with rigid body motion in this problem, we don’t want any scaling in a rotation estimate, so we’ll replace the S matrix with something like the identity matrix to remove the scaling.</li>
<li>Use the optimal rotation to get the optimal translation by aligning the centroids</li>
</ul>
</li>
<li>Estimate the uncertainty:<ul>
<li>We can obtain an estimate of the covariance matrix of the ICP solution using a formula</li>
<li>This expression tells us how the covariance of the estimated motion parameters is related to the covariance of the measurements in the two point clouds using certain second-order derivatives of the least squares cost function.</li>
</ul>
</li>
<li>Variants <ul>
<li>Point-to-point ICP minimizes the Euclidean distance between each point in $P_{S’}$, and the nearest point in $P_S$</li>
<li>Point-to-plane ICP minimizes the perpendicular distance between each point in $P_{S’}$, and the nearest plane in $P_S$·<ul>
<li>This tends to work well in structured environments like cities or indoors</li>
<li>first fit a number of planes to the first point cloud and then minimize the distance between each point in the second cloud and its closest plane in the first cloud.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Common pitfalls of the ICP algorithm</strong></p>
<ul>
<li>Outliers - Objects in Motion:<ul>
<li>be careful to exclude or mitigate the effects of outlying points that violate our assumptions of a stationary world</li>
<li>One way to do this is by fusing ICP motion estimates with GPS and INS estimates. </li>
<li>Another option is to identify and ignore moving objects, which we could do with some of the computer vision techniques.</li>
<li>But an even simpler approach for dealing with outliers like these is to choose a different loss function that is less sensitive to large errors induced by outliers than our standard squared error loss, Robust Loss Functions:</li>
</ul>
</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Learning</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Self-driving</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2019/07/31/coursera_state_est04/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
    </nav>
  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
			<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    	<div class="footer-left">
            <i class="fa fa-copyright"></i> 
            2020 Tracy Xin Wang
            
            <span class="post-meta-divider">|</span>
            <span id="busuanzi_container_site_pv" style="display: inline;">
            <i class="fa fa-eye"></i>
            <span id="busuanzi_value_site_pv"></span>
          </span>

          <span class="post-meta-divider">|</span>
          <span id="busuanzi_container_site_uv" style="display: inline;">
            <i class="fa fa-user"></i>
            <span id="busuanzi_value_site_uv"></span>
          </span>
          <span class="post-meta-divider">|</span>
					<span class="post-count">
              本站总共109.2k字
          </span>
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a>, modified by Tracy
      	</div>
    </div>
  </div>
</footer>


    </div>
    <script>
	var yiliaConfig = {
		mathjax: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: false,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Linux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Notes</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Coding</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Algorithm</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Tools</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Learning</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Graph optimization</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Brain</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">MRI</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">杂文</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">碎碎念</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">经济</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Deep learning</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Matlab</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Hexo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Neural Dynamic Simulation</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Network</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">NodeJS</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">OrangePi</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">思考</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Samba</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Stochastic</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Self-driving</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">CV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">笔记</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Python</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">learning</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">思维模型</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="http://tracyxinwang.site/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>My Personal Website</a>
            </li>
          
            <li class="search-li">
              <a href="https://johnnysun.top/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Mr. Johnny Sun</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">想&lt;br&gt;修身&lt;br&gt;齐家&lt;br&gt;兼济天下&lt;br&gt;&lt;br&gt;爱&lt;br&gt;串串&lt;br&gt;冒菜&lt;br&gt;各种折腾的四川人&lt;br&gt;&lt;br&gt;</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>